{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    make_scorer,\n",
    "    roc_curve,\n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, RepeatedStratifiedKFold"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "ap_pro_data = pd.read_csv(\"./resources/datasets/ap_pro/continuous.csv\")\n",
    "ap_pro_data"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "def show_missing_values(df: pd.DataFrame) -> None:\n",
    "    df.replace(\"?\", np.nan, inplace=True)\n",
    "    missing_values = df.isna().sum()\n",
    "    return missing_values[missing_values > 0].sort_values(ascending=False)\n",
    "\n",
    "\n",
    "show_missing_values(ap_pro_data)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "continuous_attributes = [\"Age\", \"PainDur\", \"Temp\"]  # \"WBC\"\n",
    "binary_attributes = [\"Vomiting\", \"PrevVis\", \"Guard\", \"RebTend\", \"PainShift\"]\n",
    "categorical_attributes = [\"Sex\", \"PainSite\", \"PainType\", \"TendSite\"] + binary_attributes"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "def prepare_data(df: pd.DataFrame) -> tuple[pd.DataFrame, pd.Series]:\n",
    "    df.drop(columns=[\"VisitID\", \"RecordID\", \"Observer\"], inplace=True)\n",
    "    df.replace(\"?\", np.nan, inplace=True)\n",
    "    df.dropna(axis=1, thresh=len(ap_pro_data) // 2, inplace=True)\n",
    "\n",
    "    df[\"Triage\"] = df[\"Triage\"].apply(lambda x: True if x == \"consult\" else False)\n",
    "\n",
    "    df = pd.get_dummies(df, columns=categorical_attributes)\n",
    "\n",
    "    for attribute in continuous_attributes:\n",
    "        df[attribute] = df[attribute].apply(lambda x: float(str(x).replace(\",\", \".\")))\n",
    "\n",
    "    # for attribute in binary_attributes:\n",
    "    #     df[attribute] = df[attribute].apply(lambda x: 1 if x == \"yes\" or x == \"constant\" else 0)\n",
    "\n",
    "    return df.drop(columns=[\"Triage\"]), df[\"Triage\"]\n",
    "\n",
    "\n",
    "X, y = prepare_data(ap_pro_data)\n",
    "X.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "y.value_counts()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "def show_correlations(df: pd.DataFrame) -> None:\n",
    "    plt.figure(figsize=(24, 10))\n",
    "    correlation_matrix = df.corr(method=\"kendall\")\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap=\"Blues\")\n",
    "\n",
    "\n",
    "show_correlations(pd.concat([X, y], axis=1))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "def get_baseline_classifier() -> Pipeline:\n",
    "    return Pipeline([(\"imputer\", SimpleImputer()), (\"clf\", LogisticRegression(max_iter=10000))])\n",
    "\n",
    "\n",
    "def auprc_score(y: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    precision, recall, _ = precision_recall_curve(y, y_pred)\n",
    "    return auc(recall, precision)\n",
    "\n",
    "\n",
    "def get_target_classifier(X: np.ndarray, y: np.ndarray) -> Pipeline:\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            (\"imputer\", KNNImputer()),\n",
    "            (\"smote\", SMOTE(random_state=42)),\n",
    "            (\n",
    "                \"rf\",\n",
    "                RandomForestClassifier(\n",
    "                    class_weight=\"balanced\",\n",
    "                    random_state=42,\n",
    "                    min_samples_leaf=8,\n",
    "                    min_samples_split=2,\n",
    "                    n_estimators=75,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    param_grid = {\n",
    "        # \"rf__n_estimators\": [50, 75, 100, 150, 200],\n",
    "        # \"rf__max_depth\": [None, 5, 10, 15, 20],\n",
    "        # \"rf__min_samples_split\": [2, 4, 6, 8, 10],\n",
    "        # \"rf__min_samples_leaf\": [1, 2, 4, 6, 8],\n",
    "    }\n",
    "    auprc_scorer = make_scorer(auprc_score, response_method=\"predict_proba\")\n",
    "    rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, scoring=auprc_scorer, cv=rskf, n_jobs=-1)\n",
    "    X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    return grid_search.best_estimator_"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "X, y = X.to_numpy(), y.to_numpy()\n",
    "\n",
    "baseline_classifier = get_baseline_classifier()\n",
    "target_classifier = get_target_classifier(X, y)\n",
    "\n",
    "classifiers = {\"baseline\": baseline_classifier, \"target\": target_classifier}\n",
    "\n",
    "target_classifier.fit(X, y)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "def calculate_risk_thresholds(y: np.ndarray, y_pred: np.ndarray) -> tuple[float, float]:\n",
    "    fpr, tpr, thresholds = roc_curve(y, y_pred)\n",
    "    sensitivity, specificity = tpr, 1 - fpr\n",
    "\n",
    "    medium_risk = (\n",
    "        thresholds[np.where(sensitivity >= 0.99)[0][0]] if np.any(sensitivity >= 0.99) else None\n",
    "    )\n",
    "    high_risk = (\n",
    "        thresholds[np.where(specificity >= 0.90)[0][-1]] if np.any(specificity >= 0.90) else None\n",
    "    )\n",
    "    return medium_risk, high_risk\n",
    "\n",
    "\n",
    "def calculate_rates(y: np.ndarray, y_pred: np.ndarray) -> tuple[float, float, float, float]:\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "    # positives, negatives = tp + fn, tn + fp\n",
    "\n",
    "    tpr = tp / (tp + fn)  # sensitivity\n",
    "    fnr = fn / (fn + tp)  # miss_rate\n",
    "    fpr = fp / (fp + tn)  # fall_out\n",
    "    tnr = tn / (tn + fp)  # specificity\n",
    "\n",
    "    return tpr, fnr, fpr, tnr\n",
    "\n",
    "\n",
    "def make_prediction(y_pred_proba: np.ndarray, medium_risk: float, high_risk: float) -> np.ndarray:\n",
    "    return np.array(\n",
    "        [\n",
    "            False if response < medium_risk else True if response >= high_risk else np.nan\n",
    "            for response in y_pred_proba\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def assess_classifier(X: np.ndarray, y: np.ndarray, classifier) -> dict[str, list[float]]:\n",
    "    rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "\n",
    "    auprc, auroc = [], []\n",
    "    negatives, positives, unknowns = [], [], []\n",
    "    rates = []\n",
    "    # matrices = []\n",
    "\n",
    "    for train, test in rskf.split(X, y):\n",
    "        X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_proba_train = classifier.predict_proba(X_train)[:, 1]\n",
    "        medium_risk, high_risk = calculate_risk_thresholds(y_train, y_pred_proba_train)\n",
    "\n",
    "        y_pred_proba = classifier.predict_proba(X_test)[:, 1]\n",
    "        auprc.append(auprc_score(y_test, y_pred_proba))\n",
    "        auroc.append(roc_auc_score(y_test, y_pred_proba))\n",
    "\n",
    "        predicted = make_prediction(y_pred_proba, medium_risk, high_risk)\n",
    "\n",
    "        positive = np.sum(predicted == True)\n",
    "        negative = np.sum(predicted == False)\n",
    "        unknown = np.sum(np.isnan(predicted))\n",
    "\n",
    "        number_of_samples: int = len(X_test)\n",
    "        negatives.append(negative / number_of_samples)\n",
    "        positives.append(positive / number_of_samples)\n",
    "        unknowns.append(unknown / number_of_samples)\n",
    "\n",
    "        mask = ~np.isnan(predicted)\n",
    "        rates.append(calculate_rates(y_test[mask], predicted[mask]))\n",
    "        # matrices.append(confusion_matrix(y_test[mask], predicted[mask]))\n",
    "\n",
    "    return {\n",
    "        \"auprc\": np.mean(auprc),\n",
    "        \"auroc\": np.mean(auroc),\n",
    "        \"negatives\": np.mean(negatives),\n",
    "        \"positives\": np.mean(positives),\n",
    "        \"unknowns\": np.mean(unknowns),\n",
    "        \"rates\": np.mean(rates, axis=0),\n",
    "        # \"confusion_matrix\": np.mean(matrices, axis=0),\n",
    "    }"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "results = {name: assess_classifier(X, y, classifier) for name, classifier in classifiers.items()}\n",
    "results"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "def show_result(result: dict, classifier: str) -> None:\n",
    "    two_colors, three_colors, four_colors = [\n",
    "        sns.color_palette(\"magma\", number) for number in (2, 3, 4)\n",
    "    ]\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 8), tight_layout=True)\n",
    "    ax1, ax2, ax3, ax4 = axs.flatten()\n",
    "\n",
    "    ax1.bar(\n",
    "        [\"AUPRC\", \"AUROC\"],\n",
    "        [result[\"auprc\"], result[\"auroc\"]],\n",
    "        color=two_colors,\n",
    "    )\n",
    "    ax1.grid(axis=\"y\", linestyle=\"--\", alpha=0.25)\n",
    "    for i, value in enumerate([result[\"auprc\"], result[\"auroc\"]]):\n",
    "        ax1.text(i, value, f\"{value * 100:.2f}%\", ha=\"center\", va=\"bottom\")\n",
    "    ax1.set_xlabel(\"metric\")\n",
    "    ax1.set_ylabel(\"score\")\n",
    "    ax1.set_title(\"Scores\")\n",
    "\n",
    "    ax2.bar(\n",
    "        [\"Negatives\", \"Positives\", \"Unknowns\"],\n",
    "        [result[\"negatives\"], result[\"positives\"], result[\"unknowns\"]],\n",
    "        color=three_colors,\n",
    "    )\n",
    "    ax2.grid(axis=\"y\", linestyle=\"--\", alpha=0.25)\n",
    "    for i, value in enumerate([result[\"negatives\"], result[\"positives\"], result[\"unknowns\"]]):\n",
    "        ax2.text(i, value, f\"{value * 100:.2f}%\", ha=\"center\", va=\"bottom\")\n",
    "    ax2.set_xlabel(\"class\")\n",
    "    ax2.set_ylabel(\"percentage\")\n",
    "    ax2.set_title(\"Classification Distribution\")\n",
    "\n",
    "    ax3.bar([\"TPR\", \"FNR\", \"FPR\", \"TNR\"], result[\"rates\"], color=four_colors)\n",
    "    ax3.grid(axis=\"y\", linestyle=\"--\", alpha=0.25)\n",
    "    for i, value in enumerate(result[\"rates\"]):\n",
    "        ax3.text(i, value, f\"{value * 100:.2f}%\", ha=\"center\", va=\"bottom\")\n",
    "    ax3.set_xlabel(\"type\")\n",
    "    ax3.set_ylabel(\"rate\")\n",
    "    ax3.set_title(\"Classification Rates\")\n",
    "\n",
    "    # sns.heatmap(result[\"confusion_matrix\"], annot=True, cmap=\"Blues\", ax=ax4)\n",
    "    # ax4.set_xticklabels([\"Predicted 0\", \"Predicted 1\"])\n",
    "    # ax4.set_yticklabels([\"Actual 0\", \"Actual 1\"])\n",
    "    # ax4.set_title(\"Confusion Matrix (mean)\")\n",
    "    ax4.axis(\"off\")\n",
    "\n",
    "    fig.suptitle(f\"Results for {classifier.capitalize()}\")\n",
    "    fig.savefig(f\"./resources/figures/ap_pro/{classifier}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for classifier, result in results.items():\n",
    "    show_result(result, classifier)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
