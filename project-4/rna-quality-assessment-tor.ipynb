{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projekt 4: Generalna ocena jakości modeli przestrzennych RNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# %pip install pandas biopython joblib scikit-learn catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio.PDB import PDBParser\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import make_scorer, root_mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Zapoznanie się z udostępnionymi zbiorami danych i ewentualne przetransformowanie ich do postaci ułatwiającej zastosowanie technik sztucznej inteligencji np. integracja danych składowych przechowywanych w różnych formatach z wykorzystaniem jednej spójnej reprezentacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = Path(\"./resources/datasets/RNA-Puzzles\")\n",
    "\n",
    "challenges = [f\"pz{index:02}\" for index in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>number_of_segments</th>\n",
       "      <th>number_of_residues</th>\n",
       "      <th>nucleotide_ranges</th>\n",
       "      <th>sequences</th>\n",
       "      <th>challenge_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_solution_0_rpr_A_4_C</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>A1-A8, B10-B16, B19-B23</td>\n",
       "      <td>CCGCCGCG, CAUGCCU, GGCGG</td>\n",
       "      <td>pz01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_solution_0_rpr_A_5_C</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>A1-A9, B8-B15, B18-B23</td>\n",
       "      <td>CCGCCGCGC, GCCAUGCC, UGGCGG</td>\n",
       "      <td>pz01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_solution_0_rpr_A_6_G</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>A2-A10, B7-B14, B17-B23</td>\n",
       "      <td>CGCCGCGCC, CGCCAUGC, GUGGCGG</td>\n",
       "      <td>pz01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_solution_0_rpr_A_7_C</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>A3-A11, B6-B13, B16-B22</td>\n",
       "      <td>GCCGCGCCA, GCGCCAUG, UGUGGCG</td>\n",
       "      <td>pz01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_solution_0_rpr_A_8_G</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>A4-A12, B5-B12, B15-B21</td>\n",
       "      <td>CCGCGCCAU, CGCGCCAU, CUGUGGC</td>\n",
       "      <td>pz01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>10_0_solution_4LCK_rpr_B_55_C</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>A48-A63, B16-B21, B50-B59</td>\n",
       "      <td>AGGAUAGUGAAAGCUA, UGGUAG, GGGUUCGAAU</td>\n",
       "      <td>pz10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>10_0_solution_4LCK_rpr_B_56_G</td>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>A48-A64, B15-B22, B49-B61</td>\n",
       "      <td>AGGAUAGUGAAAGCUAG, GUGGUAGA, CGGGUUCGAAUCC</td>\n",
       "      <td>pz10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>10_0_solution_4LCK_rpr_B_57_A</td>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>A49-A54, A58-A64, B14-B22, B44-B63</td>\n",
       "      <td>GGAUAG, AAGCUAG, AGUGGUAGA, GGUCGCGGGUUCGAAUCCCG</td>\n",
       "      <td>pz10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>10_0_solution_4LCK_rpr_B_58_A</td>\n",
       "      <td>4</td>\n",
       "      <td>43</td>\n",
       "      <td>A59-A63, B6-B11, B13-B23, B43-B63</td>\n",
       "      <td>AGCUA, AGUAGU, CAGUGGUAGAA, GGGUCGCGGGUUCGAAUCCCG</td>\n",
       "      <td>pz10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>10_0_solution_4LCK_rpr_B_59_U</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>B6-B10, B12-B22, B44-B52, B54-B64</td>\n",
       "      <td>AGUAG, UCAGUGGUAGA, GGUCGCGGG, UCGAAUCCCGU</td>\n",
       "      <td>pz10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>678 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          filename  number_of_segments  number_of_residues  \\\n",
       "0           1_solution_0_rpr_A_4_C                   3                  20   \n",
       "1           1_solution_0_rpr_A_5_C                   3                  23   \n",
       "2           1_solution_0_rpr_A_6_G                   3                  24   \n",
       "3           1_solution_0_rpr_A_7_C                   3                  24   \n",
       "4           1_solution_0_rpr_A_8_G                   3                  24   \n",
       "..                             ...                 ...                 ...   \n",
       "673  10_0_solution_4LCK_rpr_B_55_C                   3                  32   \n",
       "674  10_0_solution_4LCK_rpr_B_56_G                   3                  38   \n",
       "675  10_0_solution_4LCK_rpr_B_57_A                   4                  42   \n",
       "676  10_0_solution_4LCK_rpr_B_58_A                   4                  43   \n",
       "677  10_0_solution_4LCK_rpr_B_59_U                   4                  36   \n",
       "\n",
       "                      nucleotide_ranges  \\\n",
       "0               A1-A8, B10-B16, B19-B23   \n",
       "1                A1-A9, B8-B15, B18-B23   \n",
       "2               A2-A10, B7-B14, B17-B23   \n",
       "3               A3-A11, B6-B13, B16-B22   \n",
       "4               A4-A12, B5-B12, B15-B21   \n",
       "..                                  ...   \n",
       "673           A48-A63, B16-B21, B50-B59   \n",
       "674           A48-A64, B15-B22, B49-B61   \n",
       "675  A49-A54, A58-A64, B14-B22, B44-B63   \n",
       "676   A59-A63, B6-B11, B13-B23, B43-B63   \n",
       "677   B6-B10, B12-B22, B44-B52, B54-B64   \n",
       "\n",
       "                                             sequences challenge_number  \n",
       "0                             CCGCCGCG, CAUGCCU, GGCGG             pz01  \n",
       "1                          CCGCCGCGC, GCCAUGCC, UGGCGG             pz01  \n",
       "2                         CGCCGCGCC, CGCCAUGC, GUGGCGG             pz01  \n",
       "3                         GCCGCGCCA, GCGCCAUG, UGUGGCG             pz01  \n",
       "4                         CCGCGCCAU, CGCGCCAU, CUGUGGC             pz01  \n",
       "..                                                 ...              ...  \n",
       "673               AGGAUAGUGAAAGCUA, UGGUAG, GGGUUCGAAU             pz10  \n",
       "674         AGGAUAGUGAAAGCUAG, GUGGUAGA, CGGGUUCGAAUCC             pz10  \n",
       "675   GGAUAG, AAGCUAG, AGUGGUAGA, GGUCGCGGGUUCGAAUCCCG             pz10  \n",
       "676  AGCUA, AGUAGU, CAGUGGUAGAA, GGGUCGCGGGUUCGAAUCCCG             pz10  \n",
       "677         AGUAG, UCAGUGGUAGA, GGUCGCGGG, UCGAAUCCCGU             pz10  \n",
       "\n",
       "[678 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_motifs(challenges: list[str]) -> pd.DataFrame:\n",
    "    result_df = pd.DataFrame()\n",
    "    for challenge in challenges:\n",
    "        current_df = pd.read_csv(\n",
    "            DATASET_PATH / f\"{challenge}/filter-results.txt\",\n",
    "            sep=\"\\t\",\n",
    "            header=None,\n",
    "            names=[\n",
    "                \"filename\",\n",
    "                \"number_of_segments\",\n",
    "                \"number_of_residues\",\n",
    "                \"nucleotide_ranges\",\n",
    "                \"sequences\",\n",
    "            ],\n",
    "        )\n",
    "        current_df[\"challenge_number\"] = challenge\n",
    "        result_df = pd.concat([result_df, current_df])\n",
    "\n",
    "    result_df = result_df[result_df[\"number_of_segments\"] >= 3]\n",
    "    return result_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "motifs = parse_motifs(challenges)\n",
    "motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>score</th>\n",
       "      <th>solution_directory</th>\n",
       "      <th>dataset_files_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_bujnicki_1_rpr.pdb</td>\n",
       "      <td>4.769</td>\n",
       "      <td>1_solution_0_rpr_A_4_C</td>\n",
       "      <td>resources/datasets/RNA-Puzzles/pz01/1_solution...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_bujnicki_2_rpr.pdb</td>\n",
       "      <td>4.594</td>\n",
       "      <td>1_solution_0_rpr_A_4_C</td>\n",
       "      <td>resources/datasets/RNA-Puzzles/pz01/1_solution...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_bujnicki_3_rpr.pdb</td>\n",
       "      <td>3.921</td>\n",
       "      <td>1_solution_0_rpr_A_4_C</td>\n",
       "      <td>resources/datasets/RNA-Puzzles/pz01/1_solution...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_bujnicki_4_rpr.pdb</td>\n",
       "      <td>4.522</td>\n",
       "      <td>1_solution_0_rpr_A_4_C</td>\n",
       "      <td>resources/datasets/RNA-Puzzles/pz01/1_solution...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_bujnicki_5_rpr.pdb</td>\n",
       "      <td>4.616</td>\n",
       "      <td>1_solution_0_rpr_A_4_C</td>\n",
       "      <td>resources/datasets/RNA-Puzzles/pz01/1_solution...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10_DING_5_rpr.pdb</td>\n",
       "      <td>4.516</td>\n",
       "      <td>10_0_solution_4LCK_rpr_B_59_U</td>\n",
       "      <td>resources/datasets/RNA-Puzzles/pz10/10_0_solut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10_DING_6_rpr.pdb</td>\n",
       "      <td>3.939</td>\n",
       "      <td>10_0_solution_4LCK_rpr_B_59_U</td>\n",
       "      <td>resources/datasets/RNA-Puzzles/pz10/10_0_solut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10_DING_7_rpr.pdb</td>\n",
       "      <td>4.639</td>\n",
       "      <td>10_0_solution_4LCK_rpr_B_59_U</td>\n",
       "      <td>resources/datasets/RNA-Puzzles/pz10/10_0_solut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10_DING_8_rpr.pdb</td>\n",
       "      <td>5.66</td>\n",
       "      <td>10_0_solution_4LCK_rpr_B_59_U</td>\n",
       "      <td>resources/datasets/RNA-Puzzles/pz10/10_0_solut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10_DING_9_rpr.pdb</td>\n",
       "      <td>4.428</td>\n",
       "      <td>10_0_solution_4LCK_rpr_B_59_U</td>\n",
       "      <td>resources/datasets/RNA-Puzzles/pz10/10_0_solut...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20956 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                filename  score             solution_directory  \\\n",
       "0   1_bujnicki_1_rpr.pdb  4.769         1_solution_0_rpr_A_4_C   \n",
       "1   1_bujnicki_2_rpr.pdb  4.594         1_solution_0_rpr_A_4_C   \n",
       "2   1_bujnicki_3_rpr.pdb  3.921         1_solution_0_rpr_A_4_C   \n",
       "3   1_bujnicki_4_rpr.pdb  4.522         1_solution_0_rpr_A_4_C   \n",
       "4   1_bujnicki_5_rpr.pdb  4.616         1_solution_0_rpr_A_4_C   \n",
       "..                   ...    ...                            ...   \n",
       "21     10_DING_5_rpr.pdb  4.516  10_0_solution_4LCK_rpr_B_59_U   \n",
       "22     10_DING_6_rpr.pdb  3.939  10_0_solution_4LCK_rpr_B_59_U   \n",
       "23     10_DING_7_rpr.pdb  4.639  10_0_solution_4LCK_rpr_B_59_U   \n",
       "24     10_DING_8_rpr.pdb   5.66  10_0_solution_4LCK_rpr_B_59_U   \n",
       "25     10_DING_9_rpr.pdb  4.428  10_0_solution_4LCK_rpr_B_59_U   \n",
       "\n",
       "                                   dataset_files_path  \n",
       "0   resources/datasets/RNA-Puzzles/pz01/1_solution...  \n",
       "1   resources/datasets/RNA-Puzzles/pz01/1_solution...  \n",
       "2   resources/datasets/RNA-Puzzles/pz01/1_solution...  \n",
       "3   resources/datasets/RNA-Puzzles/pz01/1_solution...  \n",
       "4   resources/datasets/RNA-Puzzles/pz01/1_solution...  \n",
       "..                                                ...  \n",
       "21  resources/datasets/RNA-Puzzles/pz10/10_0_solut...  \n",
       "22  resources/datasets/RNA-Puzzles/pz10/10_0_solut...  \n",
       "23  resources/datasets/RNA-Puzzles/pz10/10_0_solut...  \n",
       "24  resources/datasets/RNA-Puzzles/pz10/10_0_solut...  \n",
       "25  resources/datasets/RNA-Puzzles/pz10/10_0_solut...  \n",
       "\n",
       "[20956 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_xml_file(file_path: Path) -> pd.DataFrame:\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"filename\": child.find(\"description\").find(\"filename\").text,\n",
    "                \"score\": child.find(\"score\").text,\n",
    "            }\n",
    "            for child in root\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def parse_scores(motifs: pd.DataFrame) -> pd.DataFrame:\n",
    "    result_df = pd.DataFrame()\n",
    "    for index, row in motifs.iterrows():\n",
    "        core_path = DATASET_PATH / f\"{row['challenge_number']}\"\n",
    "        xml_path = core_path / f\"{row['filename']}-rmsd.xml\"\n",
    "\n",
    "        if xml_path.exists():\n",
    "            current_df = parse_xml_file(xml_path)\n",
    "            current_df[\"solution_directory\"] = row[\"filename\"]\n",
    "            current_df[\"dataset_files_path\"] = core_path / (\n",
    "                f\"{row['filename']}/\" + current_df[\"filename\"].apply(lambda x: x[:-4])\n",
    "            )\n",
    "            result_df = pd.concat([result_df, current_df])\n",
    "        else:\n",
    "            motifs.drop(index, inplace=True)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "scores = parse_scores(motifs)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def parse_pdb_file(file_path: Path) -> pd.DataFrame:\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(\"PDB_structure\", f\"{file_path}.pdb\")\n",
    "\n",
    "    pdb_data: list = []\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            for residue in chain:\n",
    "                for atom in residue:\n",
    "                    pdb_data.append(\n",
    "                        {\n",
    "                            \"atom_id\": atom.serial_number,\n",
    "                            \"atom_name\": atom.name,\n",
    "                            \"residue_name\": residue.resname,\n",
    "                            \"chain_id\": chain.id,\n",
    "                            \"residue_number\": residue.id[1],\n",
    "                            \"x\": atom.coord[0],\n",
    "                            \"y\": atom.coord[1],\n",
    "                            \"z\": atom.coord[2],\n",
    "                            \"occupancy\": atom.occupancy,\n",
    "                            \"temperature_factor\": atom.bfactor,\n",
    "                        }\n",
    "                    )\n",
    "    return pd.DataFrame(pdb_data)\n",
    "\n",
    "\n",
    "def parse_tor_file(file_path: Path, keep_ids: bool = False) -> pd.DataFrame:\n",
    "    result_df = pd.read_csv(f\"{file_path}.tor\", sep=\"\\s+\")\n",
    "    return (\n",
    "        result_df.drop(columns=[\"Chain\", \"ResNum\", \"Name\", \"iCode\"])\n",
    "        if not keep_ids\n",
    "        else result_df.rename(\n",
    "            columns={\n",
    "                \"Chain\": \"chain_id\",\n",
    "                \"ResNum\": \"residue_number\",\n",
    "                \"Name\": \"residue_name\",\n",
    "                \"iCode\": \"icode\",\n",
    "            }\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Krótkie zapoznanie się z dostępnymi przestrzeniami reprezentacji struktur 3D RNA (przestrzenie kartezjańska i kątów torsyjnych) i ich formatami zapisu. Wybór obiecującej przestrzeni na której będziecie Państwo bazować wraz z uzasadnieniem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atom_id</th>\n",
       "      <th>atom_name</th>\n",
       "      <th>residue_name</th>\n",
       "      <th>chain_id</th>\n",
       "      <th>residue_number</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>occupancy</th>\n",
       "      <th>temperature_factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>C1'</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>-20.322001</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>69.922997</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>C2</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>-19.306999</td>\n",
       "      <td>-1.206</td>\n",
       "      <td>71.968002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>C2'</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>-19.379999</td>\n",
       "      <td>0.818</td>\n",
       "      <td>69.327003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>C3'</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>-19.302999</td>\n",
       "      <td>0.409</td>\n",
       "      <td>67.860001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>C4</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>-18.016001</td>\n",
       "      <td>-3.167</td>\n",
       "      <td>71.891998</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   atom_id atom_name residue_name chain_id  residue_number          x      y  \\\n",
       "0        1       C1'            C        A               1 -20.322001 -0.227   \n",
       "1        2        C2            C        A               1 -19.306999 -1.206   \n",
       "2        3       C2'            C        A               1 -19.379999  0.818   \n",
       "3        4       C3'            C        A               1 -19.302999  0.409   \n",
       "4        5        C4            C        A               1 -18.016001 -3.167   \n",
       "\n",
       "           z  occupancy  temperature_factor  \n",
       "0  69.922997        1.0                 0.0  \n",
       "1  71.968002        1.0                 0.0  \n",
       "2  69.327003        1.0                 0.0  \n",
       "3  67.860001        1.0                 0.0  \n",
       "4  71.891998        1.0                 0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_path = scores[\"dataset_files_path\"].values[0]\n",
    "example_pdb = parse_pdb_file(example_path)\n",
    "example_pdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chain_id</th>\n",
       "      <th>residue_number</th>\n",
       "      <th>icode</th>\n",
       "      <th>residue_name</th>\n",
       "      <th>alpha</th>\n",
       "      <th>beta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>delta</th>\n",
       "      <th>epsilon</th>\n",
       "      <th>zeta</th>\n",
       "      <th>chi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>C</td>\n",
       "      <td>-</td>\n",
       "      <td>-176.137</td>\n",
       "      <td>59.054</td>\n",
       "      <td>81.122</td>\n",
       "      <td>-174.56</td>\n",
       "      <td>-80.489</td>\n",
       "      <td>-145.907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>-</td>\n",
       "      <td>C</td>\n",
       "      <td>161.855</td>\n",
       "      <td>-159.318</td>\n",
       "      <td>166.249</td>\n",
       "      <td>89.951</td>\n",
       "      <td>-117.771</td>\n",
       "      <td>-80.79</td>\n",
       "      <td>-160.341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>-</td>\n",
       "      <td>G</td>\n",
       "      <td>-73.596</td>\n",
       "      <td>158.659</td>\n",
       "      <td>64.143</td>\n",
       "      <td>82.278</td>\n",
       "      <td>-147.484</td>\n",
       "      <td>-77.638</td>\n",
       "      <td>-164.295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>-</td>\n",
       "      <td>C</td>\n",
       "      <td>-75.485</td>\n",
       "      <td>171.898</td>\n",
       "      <td>62.668</td>\n",
       "      <td>83.346</td>\n",
       "      <td>-152.214</td>\n",
       "      <td>-73.751</td>\n",
       "      <td>-155.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>5</td>\n",
       "      <td>-</td>\n",
       "      <td>C</td>\n",
       "      <td>-78.552</td>\n",
       "      <td>172.551</td>\n",
       "      <td>62.180</td>\n",
       "      <td>81.483</td>\n",
       "      <td>-152.556</td>\n",
       "      <td>-73.703</td>\n",
       "      <td>-151.534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chain_id  residue_number icode residue_name    alpha     beta    gamma  \\\n",
       "0        A               1     -            C        - -176.137   59.054   \n",
       "1        A               2     -            C  161.855 -159.318  166.249   \n",
       "2        A               3     -            G  -73.596  158.659   64.143   \n",
       "3        A               4     -            C  -75.485  171.898   62.668   \n",
       "4        A               5     -            C  -78.552  172.551   62.180   \n",
       "\n",
       "    delta   epsilon     zeta      chi  \n",
       "0  81.122   -174.56  -80.489 -145.907  \n",
       "1  89.951  -117.771   -80.79 -160.341  \n",
       "2  82.278  -147.484  -77.638 -164.295  \n",
       "3  83.346  -152.214  -73.751 -155.038  \n",
       "4  81.483  -152.556  -73.703 -151.534  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tor = parse_tor_file(example_path, keep_ids=True)\n",
    "example_tor.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Określenie procentowych progów pozwalających podzielić dostępny zbiór danych na część treningową, walidacyjną i ewaluacyjną. Czy rozmiar dostępnego zbioru jest wystarczający? Czy należy go rozbudować? Jeśli tak to w jaki sposób?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# train-validation-test splits -> 70-15-15\n",
    "VALID_TEST_SIZE: float = 0.15\n",
    "TRAIN_SIZE: float = 1 - (2 * VALID_TEST_SIZE)\n",
    "\n",
    "assert TRAIN_SIZE > 0 and VALID_TEST_SIZE > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Określenie sposobu reprezentacji wiedzy, którą dysponujemy (tzn. wektora cech). Czy stosowane będą techniki identyfikacji najistotniejszych cech? Jeśli tak to jakie?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_max_tor_lines(scores: pd.DataFrame) -> int:\n",
    "    return scores[\"dataset_files_path\"].apply(lambda x: len(parse_tor_file(x))).max()\n",
    "\n",
    "\n",
    "max_tor_lines = find_max_tor_lines(scores)\n",
    "max_tor_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPRESENTATIONS_PATH = Path(\"./resources/datasets/representations\")\n",
    "TOR_DATASET_PATH = REPRESENTATIONS_PATH / \"tor_dataset.csv\"\n",
    "\n",
    "NUMBER_OF_ANGLES = 7\n",
    "FILL_FEATURES_TO = max_tor_lines * NUMBER_OF_ANGLES  # 61 * 7 = 427\n",
    "\n",
    "\n",
    "def preprocess_tor_data(tor_data: pd.DataFrame, missing_value: int = 0) -> np.ndarray:\n",
    "    tor_data = tor_data.replace(\"-\", missing_value).astype(float)\n",
    "    tor_data = tor_data.values.flatten()\n",
    "    padding_needed = max(0, FILL_FEATURES_TO - len(tor_data))\n",
    "    padded_tor_data = np.pad(\n",
    "        tor_data, pad_width=(0, padding_needed), mode=\"constant\", constant_values=missing_value\n",
    "    )\n",
    "    return padded_tor_data\n",
    "\n",
    "\n",
    "def normalize_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def normalize_column(column: pd.Series) -> pd.Series:\n",
    "        non_zero_values = column[column != 0]\n",
    "        min_val = non_zero_values.min()\n",
    "        max_val = non_zero_values.max()\n",
    "\n",
    "        column[column != 0] = (\n",
    "            2 * (non_zero_values - min_val) / (max_val - min_val) - 1 if max_val != min_val else 0\n",
    "        )\n",
    "        return column\n",
    "\n",
    "    return df.apply(normalize_column, axis=\"columns\")\n",
    "\n",
    "\n",
    "def prepare_tor_dataset(scores: pd.DataFrame) -> pd.DataFrame:\n",
    "    result_df = pd.DataFrame()\n",
    "    for _, row in scores.iterrows():\n",
    "        tor_data = parse_tor_file(row[\"dataset_files_path\"])\n",
    "        preprocessed_tor_data = preprocess_tor_data(tor_data)\n",
    "\n",
    "        current_df = pd.DataFrame(preprocessed_tor_data).T\n",
    "        current_df[\"score\"] = row[\"score\"]\n",
    "        result_df = pd.concat([result_df, current_df])\n",
    "\n",
    "    score_copy = result_df[\"score\"].copy()\n",
    "    result_df = normalize_data(result_df.drop(columns=[\"score\"]))\n",
    "    result_df[\"score\"] = score_copy\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def parallel_processing(scores: pd.DataFrame) -> pd.DataFrame:\n",
    "    cores = max(1, os.cpu_count() - 1)\n",
    "    data_splits = np.array_split(scores, cores)\n",
    "    fn = delayed(prepare_tor_dataset)\n",
    "    results = Parallel(n_jobs=cores, verbose=10)(fn(data_split) for data_split in data_splits)\n",
    "    return pd.concat(results)\n",
    "\n",
    "\n",
    "def make_tor_dataset_csv(scores: pd.DataFrame) -> None:\n",
    "    dataset = parallel_processing(scores)\n",
    "    dataset.to_csv(TOR_DATASET_PATH, index=False)\n",
    "\n",
    "\n",
    "if not TOR_DATASET_PATH.exists():\n",
    "    make_tor_dataset_csv(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>418</th>\n",
       "      <th>419</th>\n",
       "      <th>420</th>\n",
       "      <th>421</th>\n",
       "      <th>422</th>\n",
       "      <th>423</th>\n",
       "      <th>424</th>\n",
       "      <th>425</th>\n",
       "      <th>426</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.978661</td>\n",
       "      <td>0.339637</td>\n",
       "      <td>0.463333</td>\n",
       "      <td>-0.969821</td>\n",
       "      <td>-0.442532</td>\n",
       "      <td>-0.809215</td>\n",
       "      <td>0.915860</td>\n",
       "      <td>-0.884387</td>\n",
       "      <td>0.940489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.925007</td>\n",
       "      <td>0.302262</td>\n",
       "      <td>0.458506</td>\n",
       "      <td>-0.919853</td>\n",
       "      <td>-0.430515</td>\n",
       "      <td>-0.901528</td>\n",
       "      <td>-0.367817</td>\n",
       "      <td>0.971242</td>\n",
       "      <td>0.248807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.969942</td>\n",
       "      <td>0.310983</td>\n",
       "      <td>0.444815</td>\n",
       "      <td>-0.804548</td>\n",
       "      <td>-0.577312</td>\n",
       "      <td>-0.902394</td>\n",
       "      <td>0.905048</td>\n",
       "      <td>-0.871336</td>\n",
       "      <td>0.939470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.907328</td>\n",
       "      <td>0.252716</td>\n",
       "      <td>0.409614</td>\n",
       "      <td>0.987071</td>\n",
       "      <td>-0.394564</td>\n",
       "      <td>-0.753726</td>\n",
       "      <td>-0.340666</td>\n",
       "      <td>0.941621</td>\n",
       "      <td>0.283139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.960478</td>\n",
       "      <td>0.304328</td>\n",
       "      <td>0.482642</td>\n",
       "      <td>-0.788699</td>\n",
       "      <td>-0.441201</td>\n",
       "      <td>-0.924806</td>\n",
       "      <td>-0.395413</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.318126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20951</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.921765</td>\n",
       "      <td>0.350060</td>\n",
       "      <td>0.465399</td>\n",
       "      <td>-0.833842</td>\n",
       "      <td>-0.343431</td>\n",
       "      <td>-0.887391</td>\n",
       "      <td>-0.499000</td>\n",
       "      <td>0.832146</td>\n",
       "      <td>0.583481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20952</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.933915</td>\n",
       "      <td>0.485690</td>\n",
       "      <td>0.427908</td>\n",
       "      <td>-0.831433</td>\n",
       "      <td>-0.394316</td>\n",
       "      <td>-0.926236</td>\n",
       "      <td>-0.485149</td>\n",
       "      <td>0.825322</td>\n",
       "      <td>0.675647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20953</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.904442</td>\n",
       "      <td>0.422146</td>\n",
       "      <td>0.510393</td>\n",
       "      <td>-0.825556</td>\n",
       "      <td>-0.387512</td>\n",
       "      <td>-0.887714</td>\n",
       "      <td>-0.412740</td>\n",
       "      <td>0.901270</td>\n",
       "      <td>0.289614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20954</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.920129</td>\n",
       "      <td>0.353021</td>\n",
       "      <td>0.379568</td>\n",
       "      <td>-0.880728</td>\n",
       "      <td>-0.366675</td>\n",
       "      <td>-0.906016</td>\n",
       "      <td>-0.484275</td>\n",
       "      <td>0.908835</td>\n",
       "      <td>0.714776</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20955</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877129</td>\n",
       "      <td>0.390261</td>\n",
       "      <td>0.391502</td>\n",
       "      <td>-0.803104</td>\n",
       "      <td>-0.358430</td>\n",
       "      <td>-0.895301</td>\n",
       "      <td>-0.418568</td>\n",
       "      <td>0.847978</td>\n",
       "      <td>0.438714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20956 rows × 428 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6  \\\n",
       "0      0.0 -0.978661  0.339637  0.463333 -0.969821 -0.442532 -0.809215   \n",
       "1      0.0  0.925007  0.302262  0.458506 -0.919853 -0.430515 -0.901528   \n",
       "2      0.0  0.969942  0.310983  0.444815 -0.804548 -0.577312 -0.902394   \n",
       "3      0.0 -0.907328  0.252716  0.409614  0.987071 -0.394564 -0.753726   \n",
       "4      0.0  0.960478  0.304328  0.482642 -0.788699 -0.441201 -0.924806   \n",
       "...    ...       ...       ...       ...       ...       ...       ...   \n",
       "20951  0.0  0.921765  0.350060  0.465399 -0.833842 -0.343431 -0.887391   \n",
       "20952  0.0  0.933915  0.485690  0.427908 -0.831433 -0.394316 -0.926236   \n",
       "20953  0.0  0.904442  0.422146  0.510393 -0.825556 -0.387512 -0.887714   \n",
       "20954  0.0  0.920129  0.353021  0.379568 -0.880728 -0.366675 -0.906016   \n",
       "20955  0.0  0.877129  0.390261  0.391502 -0.803104 -0.358430 -0.895301   \n",
       "\n",
       "              7         8         9  ...  418  419  420  421  422  423  424  \\\n",
       "0      0.915860 -0.884387  0.940489  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1     -0.367817  0.971242  0.248807  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2      0.905048 -0.871336  0.939470  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3     -0.340666  0.941621  0.283139  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4     -0.395413  1.000000  0.318126  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "...         ...       ...       ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "20951 -0.499000  0.832146  0.583481  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "20952 -0.485149  0.825322  0.675647  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "20953 -0.412740  0.901270  0.289614  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "20954 -0.484275  0.908835  0.714776  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "20955 -0.418568  0.847978  0.438714  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "       425  426  score  \n",
       "0      0.0  0.0  4.769  \n",
       "1      0.0  0.0  4.594  \n",
       "2      0.0  0.0  3.921  \n",
       "3      0.0  0.0  4.522  \n",
       "4      0.0  0.0  4.616  \n",
       "...    ...  ...    ...  \n",
       "20951  0.0  0.0  4.516  \n",
       "20952  0.0  0.0  3.939  \n",
       "20953  0.0  0.0  4.639  \n",
       "20954  0.0  0.0  5.660  \n",
       "20955  0.0  0.0  4.428  \n",
       "\n",
       "[20956 rows x 428 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tor_dataset = pd.read_csv(TOR_DATASET_PATH)\n",
    "tor_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20956, 427), (20956,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_tor_dataset(dataset: pd.DataFrame) -> tuple[np.array, np.array]:\n",
    "    X = dataset.drop(dataset.columns[-1], axis=1)\n",
    "    y = dataset[dataset.columns[-1]]\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "X, y = preprocess_tor_dataset(tor_dataset)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Train size: (14669, 427) [70.00%]\n",
      "    Valid size: (3143, 427) [15.00%]\n",
      "    Test  size: (3144, 427) [15.00%]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE: int = 42\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, train_size=TRAIN_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "    X_valid, y_valid, test_size=0.5, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "    Train size: {X_train.shape} [{X_train.shape[0] / X.shape[0] * 100:.2f}%]\n",
    "    Valid size: {X_valid.shape} [{X_valid.shape[0] / X.shape[0] * 100:.2f}%]\n",
    "    Test  size: {X_test.shape} [{X_test.shape[0] / X.shape[0] * 100:.2f}%]\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Wybór obiecujących technik uczenia maszynowego, które uważacie Państwo, że powinny się sprawdzić podczas rozwiązywania postawionego problemu wraz z uzasadnieniem (np. głębokie sieci neuronowe, SVM, RandomForest ,itd.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "regressors = {\n",
    "    \"RandomForest\": RandomForestRegressor(n_jobs=-1, random_state=RANDOM_STATE),\n",
    "    \"MLP\": MLPRegressor(max_iter=500, random_state=RANDOM_STATE, verbose=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Iteracyjne przeprowadzenie procesu uczenia, określenie wartości parametrów kluczowych dla tego procesu (np. zastosowana funkcja straty, learning rate, optimizer, itd.) i wskazanie czy natrafiliście Państwo na jakieś problemy podczas tego procesu np. przeuczenie i jak Państwo sobie z tymi problemami poradziliście o ile rzeczywiście wystąpiły?\n",
    "\n",
    "7. Optymalizacja wartości hiperparametrów – czy warto je optymalizować w przypadku rozpatrywanego problemu? Jeśli tak to w jaki sposób?\n",
    "\n",
    "8. Wybór i uzasadnienie zastosowanych miar oceny, przeprowadzenie procesu ewaluacji uzyskanego(ych) modelu(i), podsumowanie i analiza uzyskanych wyników."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "rmse_scorer = make_scorer(root_mean_squared_error, greater_is_better=False)\n",
    "\n",
    "\n",
    "def make_prediction(Regressor, param_grid=None) -> float:\n",
    "    regr = (\n",
    "        GridSearchCV(Regressor, param_grid, scoring=rmse_scorer, n_jobs=-1, verbose=1, cv=3)\n",
    "        if param_grid\n",
    "        else Regressor\n",
    "    )\n",
    "\n",
    "    regr.fit(X_train, y_train)\n",
    "    if param_grid:\n",
    "        print(f\"Best params: {regr.best_params_}\")\n",
    "\n",
    "    y_pred = regr.predict(X_test)\n",
    "    return f\"{type(Regressor).__name__} RMSE score: {root_mean_squared_error(y_test, y_pred)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmake_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregressors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRandomForest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 11\u001b[0m, in \u001b[0;36mmake_prediction\u001b[0;34m(Regressor, param_grid)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_prediction\u001b[39m(Regressor, param_grid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m      5\u001b[0m     regr \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      6\u001b[0m         GridSearchCV(Regressor, param_grid, scoring\u001b[38;5;241m=\u001b[39mrmse_scorer, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m param_grid\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m Regressor\n\u001b[1;32m      9\u001b[0m     )\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mregr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param_grid:\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregr\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Studies/SI/SIwIB/.venv/lib/python3.9/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Studies/SI/SIwIB/.venv/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    481\u001b[0m ]\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/Studies/SI/SIwIB/.venv/lib/python3.9/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Studies/SI/SIwIB/.venv/lib/python3.9/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Studies/SI/SIwIB/.venv/lib/python3.9/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Studies/SI/SIwIB/.venv/lib/python3.9/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "make_prediction(regressors[\"RandomForest\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 45 candidates, totalling 135 fits\n",
      "Iteration 1, loss = 32.02663177\n",
      "Iteration 1, loss = 25.94309820\n",
      "Iteration 1, loss = 31.59656264\n",
      "Iteration 1, loss = 32.42123743\n",
      "Iteration 1, loss = 25.43870181\n",
      "Iteration 1, loss = 152.71913862\n",
      "Iteration 1, loss = 152.23524825\n",
      "Iteration 2, loss = 24.02253413\n",
      "Iteration 1, loss = 25.86158694\n",
      "Iteration 2, loss = 22.45863601\n",
      "Iteration 2, loss = 24.31775906\n",
      "Iteration 2, loss = 23.65377320\n",
      "Iteration 2, loss = 48.65042436\n",
      "Iteration 2, loss = 21.97425618\n",
      "Iteration 3, loss = 23.41616917\n",
      "Iteration 2, loss = 48.14100668\n",
      "Iteration 3, loss = 21.57689213\n",
      "Iteration 2, loss = 22.39737729\n",
      "Iteration 3, loss = 35.11004731\n",
      "Iteration 3, loss = 22.97036279\n",
      "Iteration 3, loss = 20.76401929\n",
      "Iteration 4, loss = 22.98150336\n",
      "Iteration 3, loss = 23.73570489\n",
      "Iteration 3, loss = 34.64132866\n",
      "Iteration 3, loss = 21.71718703\n",
      "Iteration 4, loss = 20.69115066\n",
      "Iteration 4, loss = 28.80647041\n",
      "Iteration 4, loss = 22.45401915\n",
      "Iteration 4, loss = 19.83262194\n",
      "Iteration 5, loss = 22.55694590\n",
      "Iteration 4, loss = 20.80249989\n",
      "Iteration 4, loss = 23.21035675\n",
      "Iteration 4, loss = 28.33794891\n",
      "Iteration 5, loss = 26.46824449\n",
      "Iteration 5, loss = 22.03641692\n",
      "Iteration 5, loss = 20.30373531\n",
      "Iteration 5, loss = 19.67890908\n",
      "Iteration 6, loss = 22.15653444\n",
      "Iteration 5, loss = 20.37098408\n",
      "Iteration 5, loss = 22.78247114\n",
      "Iteration 6, loss = 25.78185676\n",
      "Iteration 5, loss = 26.01361733\n",
      "Iteration 7, loss = 21.86687659\n",
      "Iteration 6, loss = 18.93563036\n",
      "Iteration 6, loss = 21.55137297\n",
      "Iteration 6, loss = 18.57521150\n",
      "Iteration 6, loss = 19.13474324\n",
      "Iteration 6, loss = 25.33031577\n",
      "Iteration 6, loss = 22.46481744\n",
      "Iteration 8, loss = 21.44372788\n",
      "Iteration 7, loss = 25.61308007\n",
      "Iteration 7, loss = 21.12959626\n",
      "Iteration 7, loss = 17.71061556\n",
      "Iteration 7, loss = 18.01579582\n",
      "Iteration 7, loss = 18.65210171\n",
      "Iteration 7, loss = 25.17029338\n",
      "Iteration 7, loss = 22.08514213\n",
      "Iteration 9, loss = 21.04389023\n",
      "Iteration 8, loss = 25.58127460\n",
      "Iteration 8, loss = 17.25551204\n",
      "Iteration 8, loss = 20.72678467\n",
      "Iteration 8, loss = 16.63285387\n",
      "Iteration 8, loss = 17.47251269\n",
      "Iteration 10, loss = 20.70686171\n",
      "Iteration 9, loss = 25.57696467\n",
      "Iteration 8, loss = 25.13937605\n",
      "Iteration 8, loss = 21.74048155\n",
      "Iteration 9, loss = 16.02499957\n",
      "Iteration 9, loss = 20.33430629\n",
      "Iteration 9, loss = 25.13644519\n",
      "Iteration 9, loss = 16.85951950\n",
      "Iteration 9, loss = 16.15589798\n",
      "Iteration 11, loss = 20.47371774\n",
      "Iteration 10, loss = 25.57728597\n",
      "Iteration 9, loss = 21.44571738\n",
      "Iteration 10, loss = 15.16440890\n",
      "Iteration 10, loss = 25.13698087\n",
      "Iteration 10, loss = 20.13026130\n",
      "Iteration 10, loss = 14.80264534\n",
      "Iteration 10, loss = 15.71249360\n",
      "Iteration 12, loss = 20.02266116\n",
      "Iteration 11, loss = 25.57840801\n",
      "Iteration 10, loss = 21.20147942\n",
      "Iteration 11, loss = 13.92007090\n",
      "Iteration 11, loss = 25.13575641Iteration 11, loss = 14.54114802\n",
      "Iteration 11, loss = 15.21449824\n",
      "\n",
      "Iteration 11, loss = 19.70038436\n",
      "Iteration 11, loss = 20.93776916\n",
      "Iteration 12, loss = 25.57694695\n",
      "Iteration 12, loss = 13.45981687\n",
      "Iteration 13, loss = 19.68395306\n",
      "Iteration 12, loss = 13.64195977\n",
      "Iteration 12, loss = 19.39373098\n",
      "Iteration 12, loss = 25.13775988\n",
      "Iteration 12, loss = 14.07891041\n",
      "Iteration 12, loss = 20.72923083\n",
      "Iteration 14, loss = 19.40891067\n",
      "Iteration 13, loss = 25.57815669\n",
      "Iteration 13, loss = 12.88710572\n",
      "Iteration 13, loss = 12.78047363\n",
      "Iteration 13, loss = 19.11474151\n",
      "Iteration 13, loss = 25.13636419\n",
      "Iteration 13, loss = 13.63350918\n",
      "Iteration 15, loss = 19.13842024\n",
      "Iteration 13, loss = 20.38820500\n",
      "Iteration 14, loss = 11.94013500\n",
      "Iteration 14, loss = 25.57745367\n",
      "Iteration 14, loss = 12.07857292\n",
      "Iteration 14, loss = 18.85710881\n",
      "Iteration 14, loss = 25.13657927\n",
      "Iteration 14, loss = 12.77830207\n",
      "Iteration 14, loss = 20.09350500\n",
      "Iteration 16, loss = 18.79546883\n",
      "Iteration 15, loss = 11.21264198\n",
      "Iteration 15, loss = 25.57912807\n",
      "Iteration 15, loss = 11.85007717\n",
      "Iteration 15, loss = 25.13993898\n",
      "Iteration 15, loss = 18.57572579\n",
      "Iteration 15, loss = 19.74202511\n",
      "Iteration 17, loss = 18.55417088\n",
      "Iteration 15, loss = 12.55830990\n",
      "Iteration 16, loss = 25.57767273\n",
      "Iteration 16, loss = 10.84205029\n",
      "Iteration 16, loss = 11.28552250\n",
      "Iteration 16, loss = 25.13530286\n",
      "Iteration 16, loss = 18.45798335\n",
      "Iteration 17, loss = 25.57937973\n",
      "Iteration 16, loss = 19.39679373\n",
      "Iteration 16, loss = 12.13630402\n",
      "Iteration 18, loss = 18.24917156\n",
      "Iteration 17, loss = 10.47435532\n",
      "Iteration 17, loss = 11.09807655\n",
      "Iteration 17, loss = 18.00583710\n",
      "Iteration 18, loss = 25.57725695\n",
      "Iteration 17, loss = 25.13616534\n",
      "Iteration 17, loss = 19.02262935\n",
      "Iteration 18, loss = 17.71226924\n",
      "Iteration 17, loss = 11.47603436\n",
      "Iteration 19, loss = 17.85220236\n",
      "Iteration 18, loss = 10.25359735\n",
      "Iteration 18, loss = 25.13706671\n",
      "Iteration 18, loss = 10.01053986\n",
      "Iteration 19, loss = 25.57792311\n",
      "Iteration 19, loss = 17.43530585\n",
      "Iteration 18, loss = 18.66927840\n",
      "Iteration 20, loss = 17.64108734\n",
      "Iteration 18, loss = 11.17417190\n",
      "Iteration 19, loss = 25.13564389\n",
      "Iteration 20, loss = 25.57776568\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 19, loss = 9.61003901\n",
      "Iteration 19, loss = 9.98602657\n",
      "Iteration 20, loss = 17.17857759\n",
      "Iteration 19, loss = 18.34900926\n",
      "Iteration 20, loss = 25.13729647\n",
      "Iteration 19, loss = 10.77161699\n",
      "Iteration 21, loss = 17.31132183\n",
      "Iteration 20, loss = 9.41521730\n",
      "Iteration 20, loss = 9.63684243\n",
      "Iteration 20, loss = 17.93434786\n",
      "Iteration 21, loss = 16.84620389\n",
      "Iteration 21, loss = 25.13838849\n",
      "Iteration 20, loss = 10.23210873\n",
      "Iteration 1, loss = 151.23456990\n",
      "Iteration 22, loss = 16.96725534\n",
      "Iteration 21, loss = 9.39440151\n",
      "Iteration 21, loss = 9.14144359\n",
      "Iteration 22, loss = 25.13814231\n",
      "Iteration 21, loss = 10.13955697\n",
      "Iteration 2, loss = 48.92959453\n",
      "Iteration 22, loss = 16.55365776\n",
      "Iteration 21, loss = 17.63154966\n",
      "Iteration 23, loss = 16.70951967\n",
      "Iteration 22, loss = 8.97524742\n",
      "Iteration 22, loss = 9.05912023\n",
      "Iteration 23, loss = 25.13569295\n",
      "Iteration 22, loss = 9.66383801\n",
      "Iteration 3, loss = 35.43648862\n",
      "Iteration 22, loss = 17.25959171\n",
      "Iteration 23, loss = 16.19469750\n",
      "Iteration 23, loss = 8.51356308\n",
      "Iteration 24, loss = 16.40040728\n",
      "Iteration 23, loss = 8.34290847\n",
      "Iteration 24, loss = 25.13664537\n",
      "Iteration 23, loss = 9.70466670\n",
      "Iteration 23, loss = 16.93782014\n",
      "Iteration 4, loss = 29.15569609\n",
      "Iteration 24, loss = 15.87570326\n",
      "Iteration 24, loss = 8.42520964\n",
      "Iteration 25, loss = 16.07127639\n",
      "Iteration 24, loss = 8.15091513\n",
      "Iteration 25, loss = 25.13640418\n",
      "Iteration 24, loss = 9.75834955\n",
      "Iteration 5, loss = 26.87464077\n",
      "Iteration 25, loss = 15.53249045\n",
      "Iteration 25, loss = 8.25169607\n",
      "Iteration 24, loss = 16.63722821\n",
      "Iteration 26, loss = 25.13559730\n",
      "Iteration 25, loss = 8.06119137\n",
      "Iteration 26, loss = 15.81260141\n",
      "Iteration 25, loss = 9.34241314\n",
      "Iteration 6, loss = 26.19610027\n",
      "Iteration 25, loss = 16.22518543\n",
      "Iteration 26, loss = 15.19106496\n",
      "Iteration 26, loss = 7.95126957\n",
      "Iteration 26, loss = 7.68998355\n",
      "Iteration 27, loss = 15.42334529\n",
      "Iteration 27, loss = 25.13715713\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 26, loss = 8.64106221\n",
      "Iteration 7, loss = 26.04375784\n",
      "Iteration 26, loss = 15.87834955\n",
      "Iteration 27, loss = 14.89606865\n",
      "Iteration 28, loss = 15.15472265\n",
      "Iteration 27, loss = 7.51458159\n",
      "Iteration 27, loss = 7.89704442\n",
      "Iteration 27, loss = 8.66201454\n",
      "Iteration 8, loss = 26.01206792\n",
      "Iteration 27, loss = 15.64178745\n",
      "Iteration 1, loss = 32.02663177\n",
      "Iteration 28, loss = 14.58344948\n",
      "Iteration 29, loss = 14.89830971\n",
      "Iteration 28, loss = 8.27186237\n",
      "Iteration 28, loss = 7.68129268\n",
      "Iteration 9, loss = 26.01171259\n",
      "Iteration 2, loss = 24.02253413\n",
      "Iteration 28, loss = 15.24589093\n",
      "Iteration 28, loss = 7.36449719\n",
      "Iteration 29, loss = 14.32255583\n",
      "Iteration 30, loss = 14.63974275\n",
      "Iteration 29, loss = 7.68400556\n",
      "Iteration 29, loss = 8.11857093\n",
      "Iteration 10, loss = 26.00905898\n",
      "Iteration 29, loss = 14.95799275\n",
      "Iteration 3, loss = 23.41616917\n",
      "Iteration 30, loss = 13.95830824\n",
      "Iteration 31, loss = 14.35137557\n",
      "Iteration 11, loss = 26.00779142\n",
      "Iteration 29, loss = 7.66513103\n",
      "Iteration 30, loss = 7.20049628\n",
      "Iteration 30, loss = 7.99410443\n",
      "Iteration 4, loss = 22.98150336\n",
      "Iteration 30, loss = 14.58781492\n",
      "Iteration 12, loss = 26.01034726\n",
      "Iteration 31, loss = 13.70493740\n",
      "Iteration 32, loss = 14.10677761\n",
      "Iteration 30, loss = 6.93663348\n",
      "Iteration 31, loss = 7.01751764\n",
      "Iteration 31, loss = 7.68387539\n",
      "Iteration 5, loss = 22.55694590\n",
      "Iteration 31, loss = 14.34993843\n",
      "Iteration 13, loss = 26.00801635\n",
      "Iteration 33, loss = 13.72537314\n",
      "Iteration 32, loss = 13.34818668\n",
      "Iteration 31, loss = 6.96362445\n",
      "Iteration 32, loss = 6.93794428\n",
      "Iteration 14, loss = 26.00806311\n",
      "Iteration 32, loss = 7.80847638\n",
      "Iteration 32, loss = 14.06312370\n",
      "Iteration 6, loss = 22.15653444\n",
      "Iteration 33, loss = 13.03483811\n",
      "Iteration 32, loss = 6.87123002\n",
      "Iteration 34, loss = 13.58954054\n",
      "Iteration 33, loss = 6.94652632\n",
      "Iteration 33, loss = 7.52322561\n",
      "Iteration 15, loss = 26.01097815\n",
      "Iteration 7, loss = 21.86687659\n",
      "Iteration 33, loss = 6.67441638\n",
      "Iteration 35, loss = 13.18961641\n",
      "Iteration 34, loss = 12.88876279\n",
      "Iteration 33, loss = 13.73171667\n",
      "Iteration 34, loss = 6.53942647\n",
      "Iteration 16, loss = 26.00861530\n",
      "Iteration 34, loss = 7.23932091\n",
      "Iteration 8, loss = 21.44372788\n",
      "Iteration 34, loss = 6.51722089\n",
      "Iteration 35, loss = 12.53235878\n",
      "Iteration 36, loss = 12.97923389\n",
      "Iteration 34, loss = 13.56257826\n",
      "Iteration 35, loss = 6.37132395\n",
      "Iteration 9, loss = 21.04389023\n",
      "Iteration 35, loss = 6.53989848\n",
      "Iteration 17, loss = 26.00731680\n",
      "Iteration 35, loss = 7.35201017\n",
      "Iteration 36, loss = 12.26646649\n",
      "Iteration 37, loss = 12.76272664\n",
      "Iteration 35, loss = 13.23524415\n",
      "Iteration 36, loss = 6.33980548\n",
      "Iteration 18, loss = 26.01190399\n",
      "Iteration 10, loss = 20.70686171\n",
      "Iteration 36, loss = 6.34935211\n",
      "Iteration 36, loss = 6.89117487\n",
      "Iteration 37, loss = 11.99937938\n",
      "Iteration 38, loss = 12.58182557\n",
      "Iteration 36, loss = 12.97637896\n",
      "Iteration 37, loss = 6.23297617\n",
      "Iteration 19, loss = 26.00805043\n",
      "Iteration 11, loss = 20.47371774\n",
      "Iteration 37, loss = 6.81632040\n",
      "Iteration 37, loss = 6.28488373\n",
      "Iteration 38, loss = 11.78949121\n",
      "Iteration 37, loss = 12.74987378\n",
      "Iteration 38, loss = 6.18885207\n",
      "Iteration 12, loss = 20.02266116\n",
      "Iteration 39, loss = 12.27695267\n",
      "Iteration 20, loss = 26.01165731\n",
      "Iteration 38, loss = 6.13313145\n",
      "Iteration 38, loss = 6.79574586\n",
      "Iteration 39, loss = 11.52318225\n",
      "Iteration 38, loss = 12.56249832\n",
      "Iteration 39, loss = 6.02788481\n",
      "Iteration 40, loss = 11.98468737\n",
      "Iteration 13, loss = 19.68395306\n",
      "Iteration 21, loss = 26.01157230\n",
      "Iteration 39, loss = 5.98694382\n",
      "Iteration 39, loss = 6.57501118\n",
      "Iteration 40, loss = 11.28371472\n",
      "Iteration 39, loss = 12.22727320\n",
      "Iteration 41, loss = 11.74549330\n",
      "Iteration 14, loss = 19.40891067\n",
      "Iteration 40, loss = 5.92549987\n",
      "Iteration 40, loss = 6.72303848\n",
      "Iteration 22, loss = 26.00744287\n",
      "Iteration 40, loss = 5.87577615\n",
      "Iteration 42, loss = 11.53575625\n",
      "Iteration 41, loss = 5.73261679\n",
      "Iteration 40, loss = 12.05548913\n",
      "Iteration 15, loss = 19.13842024\n",
      "Iteration 41, loss = 11.02462553\n",
      "Iteration 41, loss = 5.84421414\n",
      "Iteration 23, loss = 26.00719917\n",
      "Iteration 41, loss = 6.63690724\n",
      "Iteration 42, loss = 5.64585351\n",
      "Iteration 16, loss = 18.79546883\n",
      "Iteration 42, loss = 10.80077633\n",
      "Iteration 43, loss = 11.39987391\n",
      "Iteration 41, loss = 11.84079458\n",
      "Iteration 24, loss = 26.00860939\n",
      "Iteration 42, loss = 6.48284661\n",
      "Iteration 43, loss = 5.76416657\n",
      "Iteration 42, loss = 5.86132125\n",
      "Iteration 25, loss = 26.00929647\n",
      "Iteration 44, loss = 11.14193226\n",
      "Iteration 43, loss = 10.76795218\n",
      "Iteration 17, loss = 18.55417088\n",
      "Iteration 42, loss = 11.56014107\n",
      "Iteration 43, loss = 6.30307512\n",
      "Iteration 43, loss = 6.03056970\n",
      "Iteration 44, loss = 5.65108558\n",
      "Iteration 26, loss = 26.01225017\n",
      "Iteration 45, loss = 10.95947269\n",
      "Iteration 18, loss = 18.24917156\n",
      "Iteration 44, loss = 10.33643492\n",
      "Iteration 43, loss = 11.38844954\n",
      "Iteration 44, loss = 5.77360375\n",
      "Iteration 44, loss = 6.27603397\n",
      "Iteration 45, loss = 5.54421647\n",
      "Iteration 46, loss = 10.86761018\n",
      "Iteration 27, loss = 26.00844574\n",
      "Iteration 19, loss = 17.85220236\n",
      "Iteration 44, loss = 11.22134777\n",
      "Iteration 45, loss = 10.16419305\n",
      "Iteration 45, loss = 6.02482900\n",
      "Iteration 45, loss = 5.56261847\n",
      "Iteration 46, loss = 5.47909996\n",
      "Iteration 47, loss = 10.53902643\n",
      "Iteration 28, loss = 26.00911406\n",
      "Iteration 20, loss = 17.64108734\n",
      "Iteration 46, loss = 10.00001090\n",
      "Iteration 46, loss = 5.90176802\n",
      "Iteration 46, loss = 5.52312138\n",
      "Iteration 45, loss = 10.96524404\n",
      "Iteration 48, loss = 10.36134099\n",
      "Iteration 21, loss = 17.31132183\n",
      "Iteration 47, loss = 5.22341290\n",
      "Iteration 47, loss = 5.60294703\n",
      "Iteration 47, loss = 5.94991733\n",
      "Iteration 29, loss = 26.00766006\n",
      "Iteration 47, loss = 9.75557824\n",
      "Iteration 46, loss = 10.76194855\n",
      "Iteration 49, loss = 10.17986744\n",
      "Iteration 22, loss = 16.96725534\n",
      "Iteration 48, loss = 5.33350751\n",
      "Iteration 48, loss = 6.23363586\n",
      "Iteration 30, loss = 26.01188131\n",
      "Iteration 48, loss = 5.40752079\n",
      "Iteration 48, loss = 9.51761126\n",
      "Iteration 47, loss = 10.67487841\n",
      "Iteration 49, loss = 5.08139859\n",
      "Iteration 49, loss = 6.05242873\n",
      "Iteration 31, loss = 26.00957918\n",
      "Iteration 23, loss = 16.70951967\n",
      "Iteration 49, loss = 5.42039250\n",
      "Iteration 48, loss = 10.35706332\n",
      "Iteration 49, loss = 9.30315012\n",
      "Iteration 50, loss = 4.87977998\n",
      "Iteration 50, loss = 5.83716076\n",
      "Iteration 32, loss = 26.00878829\n",
      "Iteration 24, loss = 16.40040728\n",
      "Iteration 50, loss = 9.97694735\n",
      "Iteration 50, loss = 5.58670594\n",
      "Iteration 49, loss = 10.22346103\n",
      "Iteration 50, loss = 9.16899579\n",
      "Iteration 51, loss = 4.89515585\n",
      "Iteration 33, loss = 26.00836986\n",
      "Iteration 51, loss = 5.55544068\n",
      "Iteration 25, loss = 16.07127639\n",
      "Iteration 51, loss = 9.71394636\n",
      "Iteration 50, loss = 10.00370199\n",
      "Iteration 51, loss = 5.13570385\n",
      "Iteration 52, loss = 4.91920209\n",
      "Iteration 51, loss = 8.98188087\n",
      "Iteration 52, loss = 5.44835649\n",
      "Iteration 34, loss = 26.01639914\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 52, loss = 9.76376302\n",
      "Iteration 26, loss = 15.81260141\n",
      "Iteration 53, loss = 4.93902448\n",
      "Iteration 51, loss = 9.79619769\n",
      "Iteration 52, loss = 4.97600521\n",
      "Iteration 52, loss = 8.77621729\n",
      "Iteration 53, loss = 5.30429774\n",
      "Iteration 27, loss = 15.42334529\n",
      "Iteration 53, loss = 5.01379607\n",
      "Iteration 1, loss = 31.59656264\n",
      "Iteration 53, loss = 9.38888498\n",
      "Iteration 54, loss = 4.98917123\n",
      "Iteration 52, loss = 9.63572402\n",
      "Iteration 54, loss = 5.38302525\n",
      "Iteration 53, loss = 8.62506440\n",
      "Iteration 54, loss = 4.86654548\n",
      "Iteration 28, loss = 15.15472265\n",
      "Iteration 2, loss = 23.65377320\n",
      "Iteration 55, loss = 4.66805304\n",
      "Iteration 54, loss = 9.24190350\n",
      "Iteration 55, loss = 5.31583284\n",
      "Iteration 53, loss = 9.50956800\n",
      "Iteration 54, loss = 8.42046988\n",
      "Iteration 55, loss = 4.89659576\n",
      "Iteration 29, loss = 14.89830971\n",
      "Iteration 56, loss = 4.66997886\n",
      "Iteration 3, loss = 22.97036279\n",
      "Iteration 55, loss = 9.12016427\n",
      "Iteration 56, loss = 5.26117953\n",
      "Iteration 30, loss = 14.63974275\n",
      "Iteration 55, loss = 8.37325610\n",
      "Iteration 54, loss = 9.40020928\n",
      "Iteration 56, loss = 4.86516953\n",
      "Iteration 57, loss = 4.55268470\n",
      "Iteration 4, loss = 22.45401915\n",
      "Iteration 31, loss = 14.35137557\n",
      "Iteration 57, loss = 5.21028500\n",
      "Iteration 56, loss = 8.85727635\n",
      "Iteration 55, loss = 9.56888438\n",
      "Iteration 57, loss = 4.81427701\n",
      "Iteration 56, loss = 8.18687164\n",
      "Iteration 32, loss = 14.10677761\n",
      "Iteration 58, loss = 4.49464402\n",
      "Iteration 5, loss = 22.03641692\n",
      "Iteration 58, loss = 5.23337136\n",
      "Iteration 57, loss = 8.73593847\n",
      "Iteration 59, loss = 4.38814020\n",
      "Iteration 33, loss = 13.72537314\n",
      "Iteration 58, loss = 4.54158061\n",
      "Iteration 56, loss = 9.27837183\n",
      "Iteration 6, loss = 21.55137297\n",
      "Iteration 59, loss = 5.32383673\n",
      "Iteration 58, loss = 8.67189818\n",
      "Iteration 34, loss = 13.58954054\n",
      "Iteration 60, loss = 4.68779209\n",
      "Iteration 59, loss = 4.52080244\n",
      "Iteration 57, loss = 9.08647461\n",
      "Iteration 7, loss = 21.12959626\n",
      "Iteration 60, loss = 5.27308658\n",
      "Iteration 59, loss = 8.41524451\n",
      "Iteration 35, loss = 13.18961641\n",
      "Iteration 57, loss = 8.02166634\n",
      "Iteration 61, loss = 4.31313703\n",
      "Iteration 8, loss = 20.72678467\n",
      "Iteration 58, loss = 8.88629422\n",
      "Iteration 60, loss = 8.25429497\n",
      "Iteration 60, loss = 4.61056522\n",
      "Iteration 61, loss = 5.24442119\n",
      "Iteration 58, loss = 7.82224439\n",
      "Iteration 36, loss = 12.97923389\n",
      "Iteration 9, loss = 20.33430629\n",
      "Iteration 62, loss = 4.15656790\n",
      "Iteration 59, loss = 8.60611305\n",
      "Iteration 61, loss = 4.66755530\n",
      "Iteration 62, loss = 4.85128871\n",
      "Iteration 61, loss = 8.12077383\n",
      "Iteration 59, loss = 7.68778735\n",
      "Iteration 37, loss = 12.76272664\n",
      "Iteration 63, loss = 4.12160728\n",
      "Iteration 10, loss = 20.13026130\n",
      "Iteration 60, loss = 8.54889828\n",
      "Iteration 62, loss = 4.45592406\n",
      "Iteration 63, loss = 4.85553879\n",
      "Iteration 62, loss = 7.93839899\n",
      "Iteration 11, loss = 19.70038436\n",
      "Iteration 38, loss = 12.58182557\n",
      "Iteration 64, loss = 4.13339739\n",
      "Iteration 60, loss = 7.61577852\n",
      "Iteration 61, loss = 8.47881415\n",
      "Iteration 64, loss = 4.85449194\n",
      "Iteration 63, loss = 4.44542772\n",
      "Iteration 63, loss = 7.86384534\n",
      "Iteration 65, loss = 4.08997873\n",
      "Iteration 61, loss = 7.51108672\n",
      "Iteration 12, loss = 19.39373098\n",
      "Iteration 39, loss = 12.27695267\n",
      "Iteration 62, loss = 8.27093136\n",
      "Iteration 64, loss = 7.81607241\n",
      "Iteration 64, loss = 4.41744296\n",
      "Iteration 62, loss = 7.33808193\n",
      "Iteration 66, loss = 3.95943074\n",
      "Iteration 13, loss = 19.11474151\n",
      "Iteration 40, loss = 11.98468737\n",
      "Iteration 63, loss = 8.13875923\n",
      "Iteration 65, loss = 4.38052042\n",
      "Iteration 65, loss = 7.59497810\n",
      "Iteration 63, loss = 7.20330168\n",
      "Iteration 67, loss = 3.92392789\n",
      "Iteration 65, loss = 4.74727136\n",
      "Iteration 14, loss = 18.85710881\n",
      "Iteration 41, loss = 11.74549330\n",
      "Iteration 64, loss = 8.05228823\n",
      "Iteration 66, loss = 7.42858200\n",
      "Iteration 66, loss = 4.73142303\n",
      "Iteration 66, loss = 4.14135058\n",
      "Iteration 64, loss = 7.04454536\n",
      "Iteration 15, loss = 18.57572579\n",
      "Iteration 68, loss = 3.93456935\n",
      "Iteration 42, loss = 11.53575625\n",
      "Iteration 65, loss = 7.94088341\n",
      "Iteration 67, loss = 4.16186035\n",
      "Iteration 67, loss = 7.41378054\n",
      "Iteration 67, loss = 4.73884172\n",
      "Iteration 69, loss = 3.91491868\n",
      "Iteration 16, loss = 18.45798335\n",
      "Iteration 65, loss = 6.94772001\n",
      "Iteration 43, loss = 11.39987391\n",
      "Iteration 68, loss = 4.25176286\n",
      "Iteration 68, loss = 4.75157668\n",
      "Iteration 70, loss = 3.84432372\n",
      "Iteration 17, loss = 18.00583710\n",
      "Iteration 68, loss = 7.19133914\n",
      "Iteration 66, loss = 7.81668011\n",
      "Iteration 66, loss = 6.75710346\n",
      "Iteration 44, loss = 11.14193226\n",
      "Iteration 69, loss = 4.68376515\n",
      "Iteration 69, loss = 4.16684463\n",
      "Iteration 18, loss = 17.71226924\n",
      "Iteration 71, loss = 3.81301904\n",
      "Iteration 69, loss = 7.18268775\n",
      "Iteration 67, loss = 6.67108607\n",
      "Iteration 70, loss = 5.16745127\n",
      "Iteration 45, loss = 10.95947269\n",
      "Iteration 67, loss = 7.72923786\n",
      "Iteration 70, loss = 4.11918899\n",
      "Iteration 19, loss = 17.43530585\n",
      "Iteration 72, loss = 4.05099678\n",
      "Iteration 70, loss = 6.95889496\n",
      "Iteration 68, loss = 6.65701956\n",
      "Iteration 71, loss = 4.54557786\n",
      "Iteration 20, loss = 17.17857759\n",
      "Iteration 68, loss = 7.63676115\n",
      "Iteration 46, loss = 10.86761018\n",
      "Iteration 73, loss = 3.76592254\n",
      "Iteration 71, loss = 6.77179570\n",
      "Iteration 71, loss = 4.19554637\n",
      "Iteration 69, loss = 6.49273490\n",
      "Iteration 21, loss = 16.84620389\n",
      "Iteration 69, loss = 7.47999517\n",
      "Iteration 74, loss = 3.73915749\n",
      "Iteration 47, loss = 10.53902643\n",
      "Iteration 72, loss = 4.39616687\n",
      "Iteration 72, loss = 6.72888960\n",
      "Iteration 72, loss = 4.10358090\n",
      "Iteration 22, loss = 16.55365776\n",
      "Iteration 70, loss = 6.42703908\n",
      "Iteration 75, loss = 3.67722719\n",
      "Iteration 70, loss = 7.38297579\n",
      "Iteration 73, loss = 4.76372350\n",
      "Iteration 48, loss = 10.36134099\n",
      "Iteration 73, loss = 6.55533642\n",
      "Iteration 23, loss = 16.19469750\n",
      "Iteration 73, loss = 4.20150343\n",
      "Iteration 71, loss = 6.25414906\n",
      "Iteration 71, loss = 7.30453110\n",
      "Iteration 76, loss = 3.54485595\n",
      "Iteration 74, loss = 4.44505247\n",
      "Iteration 49, loss = 10.17986744\n",
      "Iteration 24, loss = 15.87570326\n",
      "Iteration 74, loss = 6.53803425\n",
      "Iteration 74, loss = 3.98038374\n",
      "Iteration 72, loss = 7.27794131\n",
      "Iteration 77, loss = 3.61059319\n",
      "Iteration 72, loss = 6.18536023\n",
      "Iteration 75, loss = 4.50222820\n",
      "Iteration 50, loss = 9.97694735\n",
      "Iteration 25, loss = 15.53249045\n",
      "Iteration 75, loss = 6.37021941\n",
      "Iteration 75, loss = 3.87755853\n",
      "Iteration 73, loss = 7.09307329\n",
      "Iteration 78, loss = 3.52342236\n",
      "Iteration 76, loss = 4.29693741\n",
      "Iteration 73, loss = 6.09735385\n",
      "Iteration 26, loss = 15.19106496\n",
      "Iteration 51, loss = 9.71394636\n",
      "Iteration 76, loss = 3.84594101\n",
      "Iteration 74, loss = 7.08390078\n",
      "Iteration 76, loss = 6.31748739\n",
      "Iteration 79, loss = 3.50862325\n",
      "Iteration 74, loss = 6.01540972\n",
      "Iteration 77, loss = 4.16530854\n",
      "Iteration 27, loss = 14.89606865\n",
      "Iteration 52, loss = 9.76376302\n",
      "Iteration 77, loss = 3.79710138\n",
      "Iteration 75, loss = 6.93420295\n",
      "Iteration 77, loss = 6.25218410\n",
      "Iteration 80, loss = 3.22066450\n",
      "Iteration 78, loss = 4.30038010\n",
      "Iteration 75, loss = 5.89980010\n",
      "Iteration 28, loss = 14.58344948\n",
      "Iteration 76, loss = 6.83615553\n",
      "Iteration 78, loss = 3.84056538\n",
      "Iteration 81, loss = 3.39328737\n",
      "Iteration 78, loss = 6.08287096\n",
      "Iteration 53, loss = 9.38888498\n",
      "Iteration 79, loss = 4.53846758\n",
      "Iteration 76, loss = 5.79254317\n",
      "Iteration 29, loss = 14.32255583\n",
      "Iteration 82, loss = 3.39519409\n",
      "Iteration 77, loss = 6.69457826\n",
      "Iteration 79, loss = 3.70863136\n",
      "Iteration 54, loss = 9.24190350\n",
      "Iteration 79, loss = 6.07994713\n",
      "Iteration 80, loss = 4.73769731\n",
      "Iteration 77, loss = 5.68515108\n",
      "Iteration 80, loss = 3.67401564\n",
      "Iteration 30, loss = 13.95830824\n",
      "Iteration 55, loss = 9.12016427\n",
      "Iteration 83, loss = 3.45681193\n",
      "Iteration 78, loss = 6.56569313\n",
      "Iteration 80, loss = 5.97530822\n",
      "Iteration 78, loss = 5.57749821\n",
      "Iteration 81, loss = 4.49968241\n",
      "Iteration 81, loss = 3.72628508\n",
      "Iteration 56, loss = 8.85727635\n",
      "Iteration 31, loss = 13.70493740\n",
      "Iteration 84, loss = 3.23920771\n",
      "Iteration 79, loss = 6.63235333\n",
      "Iteration 81, loss = 5.77660600\n",
      "Iteration 82, loss = 3.90760734\n",
      "Iteration 79, loss = 5.57443489\n",
      "Iteration 32, loss = 13.34818668\n",
      "Iteration 82, loss = 3.65041308\n",
      "Iteration 85, loss = 3.50564825\n",
      "Iteration 57, loss = 8.73593847\n",
      "Iteration 82, loss = 5.70036492\n",
      "Iteration 80, loss = 6.43456757\n",
      "Iteration 83, loss = 4.22443952\n",
      "Iteration 83, loss = 3.55472537\n",
      "Iteration 81, loss = 6.36328815\n",
      "Iteration 83, loss = 5.61043507\n",
      "Iteration 33, loss = 13.03483811\n",
      "Iteration 58, loss = 8.67189818\n",
      "Iteration 80, loss = 5.48798648\n",
      "Iteration 86, loss = 3.28956269\n",
      "Iteration 84, loss = 3.81108401\n",
      "Iteration 84, loss = 5.56296662\n",
      "Iteration 34, loss = 12.88876279\n",
      "Iteration 84, loss = 3.50025696\n",
      "Iteration 81, loss = 5.35171288\n",
      "Iteration 87, loss = 3.12458099\n",
      "Iteration 82, loss = 6.21382293\n",
      "Iteration 59, loss = 8.41524451\n",
      "Iteration 85, loss = 3.79633225\n",
      "Iteration 82, loss = 5.28038068\n",
      "Iteration 85, loss = 3.49059406\n",
      "Iteration 85, loss = 5.51879854\n",
      "Iteration 88, loss = 3.00531634\n",
      "Iteration 60, loss = 8.25429497\n",
      "Iteration 83, loss = 6.23080684\n",
      "Iteration 35, loss = 12.53235878\n",
      "Iteration 86, loss = 3.80895825\n",
      "Iteration 83, loss = 5.23547861\n",
      "Iteration 89, loss = 3.17286353\n",
      "Iteration 84, loss = 6.06982974\n",
      "Iteration 86, loss = 3.43165610\n",
      "Iteration 61, loss = 8.12077383\n",
      "Iteration 36, loss = 12.26646649\n",
      "Iteration 86, loss = 5.40450510\n",
      "Iteration 87, loss = 3.89071625\n",
      "Iteration 62, loss = 7.93839899\n",
      "Iteration 84, loss = 5.25766142\n",
      "Iteration 87, loss = 5.45303316\n",
      "Iteration 90, loss = 3.05532063\n",
      "Iteration 87, loss = 3.57220879\n",
      "Iteration 37, loss = 11.99937938\n",
      "Iteration 85, loss = 5.99281907\n",
      "Iteration 88, loss = 3.72793362\n",
      "Iteration 63, loss = 7.86384534\n",
      "Iteration 86, loss = 5.98834639\n",
      "Iteration 85, loss = 5.07514827\n",
      "Iteration 88, loss = 3.48398735\n",
      "Iteration 88, loss = 5.26282481\n",
      "Iteration 91, loss = 3.16320743\n",
      "Iteration 38, loss = 11.78949121\n",
      "Iteration 89, loss = 3.77554761\n",
      "Iteration 64, loss = 7.81607241\n",
      "Iteration 87, loss = 5.89009255\n",
      "Iteration 86, loss = 5.04641757\n",
      "Iteration 89, loss = 3.97875605\n",
      "Iteration 92, loss = 3.18424202\n",
      "Iteration 89, loss = 5.26389624\n",
      "Iteration 39, loss = 11.52318225\n",
      "Iteration 65, loss = 7.59497810\n",
      "Iteration 88, loss = 5.85368683\n",
      "Iteration 90, loss = 3.78560339\n",
      "Iteration 87, loss = 4.89007696\n",
      "Iteration 90, loss = 3.69282100\n",
      "Iteration 40, loss = 11.28371472\n",
      "Iteration 90, loss = 5.11800471\n",
      "Iteration 93, loss = 3.21203777\n",
      "Iteration 66, loss = 7.42858200\n",
      "Iteration 91, loss = 3.80237844\n",
      "Iteration 89, loss = 5.70391941\n",
      "Iteration 88, loss = 4.89999979\n",
      "Iteration 91, loss = 3.34513832\n",
      "Iteration 91, loss = 5.03047612\n",
      "Iteration 94, loss = 3.21481369\n",
      "Iteration 41, loss = 11.02462553\n",
      "Iteration 92, loss = 3.83191312\n",
      "Iteration 90, loss = 5.63755677\n",
      "Iteration 67, loss = 7.41378054\n",
      "Iteration 92, loss = 4.99629755\n",
      "Iteration 89, loss = 4.83383739\n",
      "Iteration 95, loss = 3.23947507\n",
      "Iteration 42, loss = 10.80077633\n",
      "Iteration 92, loss = 3.51353440\n",
      "Iteration 68, loss = 7.19133914\n",
      "Iteration 93, loss = 3.71526922\n",
      "Iteration 90, loss = 4.81353052\n",
      "Iteration 91, loss = 5.58269479\n",
      "Iteration 43, loss = 10.76795218\n",
      "Iteration 93, loss = 4.94411400\n",
      "Iteration 93, loss = 3.36161505\n",
      "Iteration 96, loss = 3.11209802\n",
      "Iteration 69, loss = 7.18268775\n",
      "Iteration 94, loss = 3.63508294\n",
      "Iteration 91, loss = 4.70776060\n",
      "Iteration 92, loss = 5.56265820\n",
      "Iteration 94, loss = 3.17186758\n",
      "Iteration 44, loss = 10.33643492\n",
      "Iteration 94, loss = 5.01503491\n",
      "Iteration 97, loss = 3.07811951\n",
      "Iteration 70, loss = 6.95889496\n",
      "Iteration 95, loss = 3.57704163\n",
      "Iteration 92, loss = 4.60273984\n",
      "Iteration 95, loss = 3.29537501\n",
      "Iteration 93, loss = 5.47395707\n",
      "Iteration 45, loss = 10.16419305\n",
      "Iteration 95, loss = 4.84587821\n",
      "Iteration 71, loss = 6.77179570\n",
      "Iteration 98, loss = 3.04537255\n",
      "Iteration 96, loss = 3.78058958\n",
      "Iteration 93, loss = 4.52302958\n",
      "Iteration 96, loss = 3.18531727\n",
      "Iteration 94, loss = 5.36323762\n",
      "Iteration 46, loss = 10.00001090\n",
      "Iteration 96, loss = 4.70720034\n",
      "Iteration 72, loss = 6.72888960\n",
      "Iteration 97, loss = 3.54689308\n",
      "Iteration 99, loss = 2.89896084\n",
      "Iteration 94, loss = 4.47591977\n",
      "Iteration 97, loss = 3.25128864\n",
      "Iteration 95, loss = 5.32636942\n",
      "Iteration 98, loss = 3.47551807\n",
      "Iteration 47, loss = 9.75557824\n",
      "Iteration 97, loss = 4.71301713\n",
      "Iteration 100, loss = 3.01455255\n",
      "Iteration 73, loss = 6.55533642\n",
      "Iteration 99, loss = 3.39779817\n",
      "Iteration 98, loss = 3.09638945\n",
      "Iteration 95, loss = 4.46547728\n",
      "Iteration 48, loss = 9.51761126\n",
      "Iteration 96, loss = 5.24782936\n",
      "Iteration 100, loss = 3.39029227\n",
      "Iteration 98, loss = 4.61781483\n",
      "Iteration 101, loss = 2.80571855\n",
      "Iteration 74, loss = 6.53803425\n",
      "Iteration 99, loss = 3.23543493\n",
      "Iteration 96, loss = 4.44653944\n",
      "Iteration 101, loss = 3.55171159\n",
      "Iteration 97, loss = 5.16083112\n",
      "Iteration 49, loss = 9.30315012\n",
      "Iteration 102, loss = 2.89533128\n",
      "Iteration 99, loss = 4.55672735\n",
      "Iteration 75, loss = 6.37021941\n",
      "Iteration 100, loss = 3.22686437\n",
      "Iteration 98, loss = 5.17953887\n",
      "Iteration 97, loss = 4.31403810\n",
      "Iteration 102, loss = 3.48587321\n",
      "Iteration 103, loss = 3.04500474\n",
      "Iteration 50, loss = 9.16899579\n",
      "Iteration 100, loss = 4.52601196\n",
      "Iteration 76, loss = 6.31748739\n",
      "Iteration 101, loss = 3.04484949\n",
      "Iteration 99, loss = 5.09077703\n",
      "Iteration 103, loss = 4.06712203\n",
      "Iteration 98, loss = 4.29359886\n",
      "Iteration 104, loss = 2.80504100\n",
      "Iteration 51, loss = 8.98188087\n",
      "Iteration 77, loss = 6.25218410\n",
      "Iteration 101, loss = 4.47868368\n",
      "Iteration 102, loss = 3.09989142\n",
      "Iteration 104, loss = 3.59154894\n",
      "Iteration 100, loss = 5.02570189\n",
      "Iteration 99, loss = 4.35873888\n",
      "Iteration 105, loss = 2.78913183\n",
      "Iteration 52, loss = 8.77621729\n",
      "Iteration 78, loss = 6.08287096\n",
      "Iteration 102, loss = 4.42270512\n",
      "Iteration 103, loss = 2.97275613\n",
      "Iteration 101, loss = 4.95029995\n",
      "Iteration 105, loss = 3.35466660\n",
      "Iteration 100, loss = 4.21776829\n",
      "Iteration 106, loss = 2.89790367\n",
      "Iteration 53, loss = 8.62506440\n",
      "Iteration 79, loss = 6.07994713\n",
      "Iteration 103, loss = 4.41938241\n",
      "Iteration 104, loss = 2.96180606\n",
      "Iteration 106, loss = 3.24351483\n",
      "Iteration 102, loss = 4.88343644\n",
      "Iteration 107, loss = 3.13617925\n",
      "Iteration 101, loss = 4.10375034\n",
      "Iteration 54, loss = 8.42046988\n",
      "Iteration 80, loss = 5.97530822\n",
      "Iteration 107, loss = 3.34996437\n",
      "Iteration 105, loss = 3.12166147\n",
      "Iteration 104, loss = 4.30755339\n",
      "Iteration 108, loss = 2.89249299\n",
      "Iteration 103, loss = 4.78951785\n",
      "Iteration 55, loss = 8.37325610\n",
      "Iteration 81, loss = 5.77660600\n",
      "Iteration 105, loss = 4.28866065\n",
      "Iteration 108, loss = 3.34010056\n",
      "Iteration 109, loss = 3.02759151\n",
      "Iteration 104, loss = 4.78868077\n",
      "Iteration 106, loss = 2.97372550\n",
      "Iteration 56, loss = 8.18687164\n",
      "Iteration 102, loss = 4.07871953\n",
      "Iteration 82, loss = 5.70036492\n",
      "Iteration 106, loss = 4.17978561\n",
      "Iteration 109, loss = 3.33582314\n",
      "Iteration 110, loss = 2.85202293\n",
      "Iteration 107, loss = 3.23219944\n",
      "Iteration 105, loss = 4.86762479\n",
      "Iteration 57, loss = 8.02166634\n",
      "Iteration 107, loss = 4.15688392\n",
      "Iteration 83, loss = 5.61043507\n",
      "Iteration 103, loss = 4.02145187\n",
      "Iteration 108, loss = 2.94411462\n",
      "Iteration 111, loss = 2.85582419\n",
      "Iteration 110, loss = 3.18351169\n",
      "Iteration 106, loss = 4.73468102\n",
      "Iteration 58, loss = 7.82224439\n",
      "Iteration 108, loss = 4.20395447\n",
      "Iteration 84, loss = 5.56296662\n",
      "Iteration 109, loss = 2.78690378\n",
      "Iteration 104, loss = 3.96543749\n",
      "Iteration 107, loss = 4.64498452\n",
      "Iteration 112, loss = 2.64149401\n",
      "Iteration 59, loss = 7.68778735\n",
      "Iteration 111, loss = 3.39765042\n",
      "Iteration 109, loss = 4.07098806\n",
      "Iteration 85, loss = 5.51879854\n",
      "Iteration 110, loss = 2.90355082\n",
      "Iteration 105, loss = 3.94458635\n",
      "Iteration 113, loss = 2.66913272\n",
      "Iteration 108, loss = 4.59344619\n",
      "Iteration 60, loss = 7.61577852\n",
      "Iteration 112, loss = 3.28472414\n",
      "Iteration 111, loss = 2.81731108\n",
      "Iteration 86, loss = 5.40450510\n",
      "Iteration 110, loss = 4.07203324\n",
      "Iteration 114, loss = 2.57438553\n",
      "Iteration 109, loss = 4.56982164\n",
      "Iteration 61, loss = 7.51108672\n",
      "Iteration 106, loss = 3.90488486\n",
      "Iteration 113, loss = 3.15429878\n",
      "Iteration 87, loss = 5.45303316\n",
      "Iteration 112, loss = 2.87733229\n",
      "Iteration 111, loss = 3.97180628\n",
      "Iteration 62, loss = 7.33808193\n",
      "Iteration 115, loss = 2.52140726\n",
      "Iteration 110, loss = 4.51993903\n",
      "Iteration 114, loss = 3.25748906\n",
      "Iteration 107, loss = 3.90314136\n",
      "Iteration 113, loss = 3.10374106\n",
      "Iteration 112, loss = 3.92699919\n",
      "Iteration 88, loss = 5.26282481\n",
      "Iteration 116, loss = 2.82636638\n",
      "Iteration 111, loss = 4.43323184\n",
      "Iteration 63, loss = 7.20330168\n",
      "Iteration 108, loss = 3.88668390\n",
      "Iteration 115, loss = 3.59492204\n",
      "Iteration 113, loss = 3.87752839\n",
      "Iteration 89, loss = 5.26389624\n",
      "Iteration 114, loss = 2.88018793\n",
      "Iteration 117, loss = 2.69646611\n",
      "Iteration 112, loss = 4.49858253\n",
      "Iteration 109, loss = 3.76405718\n",
      "Iteration 64, loss = 7.04454536\n",
      "Iteration 114, loss = 3.84410349\n",
      "Iteration 116, loss = 3.30929507\n",
      "Iteration 90, loss = 5.11800471\n",
      "Iteration 115, loss = 2.79098814\n",
      "Iteration 110, loss = 3.73347808\n",
      "Iteration 113, loss = 4.43096206\n",
      "Iteration 118, loss = 2.65023181\n",
      "Iteration 65, loss = 6.94772001\n",
      "Iteration 115, loss = 3.80274031\n",
      "Iteration 117, loss = 3.25085832\n",
      "Iteration 91, loss = 5.03047612\n",
      "Iteration 114, loss = 4.29071924\n",
      "Iteration 111, loss = 3.74358900\n",
      "Iteration 66, loss = 6.75710346\n",
      "Iteration 118, loss = 3.19594250\n",
      "Iteration 119, loss = 2.73902633\n",
      "Iteration 116, loss = 3.77363008\n",
      "Iteration 92, loss = 4.99629755\n",
      "Iteration 115, loss = 4.37336910\n",
      "Iteration 112, loss = 3.64740293\n",
      "Iteration 116, loss = 2.82976168\n",
      "Iteration 117, loss = 3.75880313\n",
      "Iteration 120, loss = 2.53952230\n",
      "Iteration 119, loss = 3.10057011\n",
      "Iteration 67, loss = 6.67108607\n",
      "Iteration 93, loss = 4.94411400\n",
      "Iteration 116, loss = 4.24950271\n",
      "Iteration 117, loss = 2.87172220\n",
      "Iteration 120, loss = 3.11883962\n",
      "Iteration 113, loss = 3.68913305\n",
      "Iteration 118, loss = 3.74107064\n",
      "Iteration 121, loss = 2.44349657\n",
      "Iteration 68, loss = 6.65701956\n",
      "Iteration 118, loss = 2.82184587\n",
      "Iteration 94, loss = 5.01503491\n",
      "Iteration 117, loss = 4.27708940\n",
      "Iteration 69, loss = 6.49273490\n",
      "Iteration 121, loss = 3.01349217\n",
      "Iteration 122, loss = 2.47964542\n",
      "Iteration 119, loss = 3.74163136\n",
      "Iteration 114, loss = 3.65360100\n",
      "Iteration 119, loss = 2.65760684\n",
      "Iteration 95, loss = 4.84587821\n",
      "Iteration 122, loss = 2.95596227\n",
      "Iteration 70, loss = 6.42703908\n",
      "Iteration 123, loss = 2.48705109\n",
      "Iteration 118, loss = 4.18189786\n",
      "Iteration 120, loss = 3.64634445\n",
      "Iteration 115, loss = 3.55351298\n",
      "Iteration 96, loss = 4.70720034\n",
      "Iteration 120, loss = 2.77142101\n",
      "Iteration 71, loss = 6.25414906\n",
      "Iteration 121, loss = 3.55907463\n",
      "Iteration 119, loss = 4.13213497\n",
      "Iteration 124, loss = 2.38977439\n",
      "Iteration 123, loss = 3.43318350\n",
      "Iteration 116, loss = 3.49677664\n",
      "Iteration 122, loss = 3.60969163\n",
      "Iteration 125, loss = 2.51550797\n",
      "Iteration 124, loss = 3.23880664\n",
      "Iteration 97, loss = 4.71301713\n",
      "Iteration 121, loss = 3.09321610\n",
      "Iteration 72, loss = 6.18536023\n",
      "Iteration 120, loss = 4.07143192\n",
      "Iteration 73, loss = 6.09735385\n",
      "Iteration 126, loss = 2.47393904\n",
      "Iteration 125, loss = 3.19018314\n",
      "Iteration 98, loss = 4.61781483\n",
      "Iteration 121, loss = 4.09965316\n",
      "Iteration 117, loss = 3.50347462\n",
      "Iteration 123, loss = 3.52837130\n",
      "Iteration 122, loss = 2.84841232\n",
      "Iteration 127, loss = 2.45956678\n",
      "Iteration 126, loss = 2.93122153\n",
      "Iteration 74, loss = 6.01540972\n",
      "Iteration 118, loss = 3.45708639\n",
      "Iteration 122, loss = 4.13142598\n",
      "Iteration 99, loss = 4.55672735\n",
      "Iteration 128, loss = 2.40550631\n",
      "Iteration 127, loss = 2.99709119\n",
      "Iteration 75, loss = 5.89980010\n",
      "Iteration 123, loss = 2.55666446\n",
      "Iteration 119, loss = 3.38938410\n",
      "Iteration 124, loss = 3.59775309\n",
      "Iteration 123, loss = 4.02487994\n",
      "Iteration 100, loss = 4.52601196\n",
      "Iteration 129, loss = 2.38332166\n",
      "Iteration 124, loss = 2.67407583\n",
      "Iteration 128, loss = 2.99005350\n",
      "Iteration 76, loss = 5.79254317\n",
      "Iteration 120, loss = 3.43469328\n",
      "Iteration 124, loss = 4.00230065\n",
      "Iteration 125, loss = 3.48647726\n",
      "Iteration 101, loss = 4.47868368\n",
      "Iteration 130, loss = 2.31854527\n",
      "Iteration 125, loss = 2.68128980\n",
      "Iteration 121, loss = 3.41739851\n",
      "Iteration 77, loss = 5.68515108\n",
      "Iteration 129, loss = 2.89541483\n",
      "Iteration 125, loss = 3.93797551\n",
      "Iteration 102, loss = 4.42270512\n",
      "Iteration 131, loss = 2.37384510\n",
      "Iteration 126, loss = 3.45914378\n",
      "Iteration 126, loss = 2.80224108\n",
      "Iteration 78, loss = 5.57749821\n",
      "Iteration 122, loss = 3.33042849\n",
      "Iteration 130, loss = 2.87027882\n",
      "Iteration 103, loss = 4.41938241\n",
      "Iteration 132, loss = 2.50919647\n",
      "Iteration 126, loss = 3.87119367\n",
      "Iteration 127, loss = 3.54994530\n",
      "Iteration 127, loss = 2.96503073\n",
      "Iteration 123, loss = 3.24381223\n",
      "Iteration 79, loss = 5.57443489\n",
      "Iteration 104, loss = 4.30755339\n",
      "Iteration 127, loss = 3.87163140\n",
      "Iteration 133, loss = 2.41121348\n",
      "Iteration 128, loss = 3.41179620\n",
      "Iteration 131, loss = 3.18037445\n",
      "Iteration 128, loss = 2.66324426\n",
      "Iteration 124, loss = 3.27928946\n",
      "Iteration 80, loss = 5.48798648\n",
      "Iteration 105, loss = 4.28866065\n",
      "Iteration 132, loss = 3.11963584\n",
      "Iteration 134, loss = 2.35012959\n",
      "Iteration 128, loss = 4.00896974\n",
      "Iteration 129, loss = 3.30943820\n",
      "Iteration 129, loss = 2.66027261\n",
      "Iteration 125, loss = 3.24884259\n",
      "Iteration 81, loss = 5.35171288\n",
      "Iteration 133, loss = 3.12153052\n",
      "Iteration 135, loss = 2.23816017\n",
      "Iteration 130, loss = 2.48213032\n",
      "Iteration 130, loss = 3.28288856\n",
      "Iteration 129, loss = 3.80809499\n",
      "Iteration 106, loss = 4.17978561\n",
      "Iteration 126, loss = 3.26243541\n",
      "Iteration 134, loss = 3.17825794\n",
      "Iteration 82, loss = 5.28038068\n",
      "Iteration 131, loss = 2.77962336\n",
      "Iteration 136, loss = 2.19500065\n",
      "Iteration 130, loss = 3.72247821\n",
      "Iteration 107, loss = 4.15688392\n",
      "Iteration 131, loss = 3.29172913\n",
      "Iteration 127, loss = 3.20299017\n",
      "Iteration 83, loss = 5.23547861\n",
      "Iteration 135, loss = 3.12794291\n",
      "Iteration 132, loss = 2.58242868\n",
      "Iteration 137, loss = 2.65595357\n",
      "Iteration 131, loss = 3.72431086\n",
      "Iteration 108, loss = 4.20395447\n",
      "Iteration 132, loss = 3.25840618\n",
      "Iteration 128, loss = 3.16433924\n",
      "Iteration 136, loss = 2.78161375\n",
      "Iteration 133, loss = 2.62989942\n",
      "Iteration 84, loss = 5.25766142\n",
      "Iteration 138, loss = 2.31561276\n",
      "Iteration 132, loss = 3.69714115\n",
      "Iteration 133, loss = 3.20355107\n",
      "Iteration 109, loss = 4.07098806\n",
      "Iteration 137, loss = 2.88133340\n",
      "Iteration 134, loss = 2.67423345\n",
      "Iteration 129, loss = 3.18935832\n",
      "Iteration 139, loss = 2.20136357\n",
      "Iteration 85, loss = 5.07514827\n",
      "Iteration 133, loss = 3.74428857\n",
      "Iteration 138, loss = 2.77456205\n",
      "Iteration 134, loss = 3.22956873\n",
      "Iteration 135, loss = 2.77114015\n",
      "Iteration 110, loss = 4.07203324\n",
      "Iteration 130, loss = 3.11181300\n",
      "Iteration 86, loss = 5.04641757\n",
      "Iteration 140, loss = 2.16287434\n",
      "Iteration 134, loss = 3.77111155\n",
      "Iteration 136, loss = 2.64471377\n",
      "Iteration 139, loss = 2.94798559\n",
      "Iteration 135, loss = 3.22918730\n",
      "Iteration 111, loss = 3.97180628\n",
      "Iteration 131, loss = 3.06340530\n",
      "Iteration 87, loss = 4.89007696\n",
      "Iteration 141, loss = 2.13088999\n",
      "Iteration 135, loss = 3.61500578\n",
      "Iteration 137, loss = 2.56448980\n",
      "Iteration 140, loss = 2.94480482\n",
      "Iteration 136, loss = 3.14101411\n",
      "Iteration 88, loss = 4.89999979\n",
      "Iteration 132, loss = 3.07892908\n",
      "Iteration 142, loss = 2.32989068\n",
      "Iteration 136, loss = 3.54943593\n",
      "Iteration 112, loss = 3.92699919\n",
      "Iteration 138, loss = 2.49262889\n",
      "Iteration 89, loss = 4.83383739\n",
      "Iteration 133, loss = 3.15892450\n",
      "Iteration 137, loss = 3.13382765\n",
      "Iteration 137, loss = 3.64165561\n",
      "Iteration 141, loss = 2.90434368\n",
      "Iteration 143, loss = 2.24249021\n",
      "Iteration 139, loss = 2.54774296\n",
      "Iteration 113, loss = 3.87752839\n",
      "Iteration 90, loss = 4.81353052\n",
      "Iteration 138, loss = 3.52412795\n",
      "Iteration 138, loss = 3.11959175\n",
      "Iteration 134, loss = 3.08952062\n",
      "Iteration 142, loss = 2.86996023\n",
      "Iteration 144, loss = 2.30890275\n",
      "Iteration 140, loss = 2.54296685\n",
      "Iteration 114, loss = 3.84410349\n",
      "Iteration 91, loss = 4.70776060\n",
      "Iteration 139, loss = 3.76240739\n",
      "Iteration 139, loss = 3.13757660\n",
      "Iteration 135, loss = 3.02862751\n",
      "Iteration 141, loss = 2.56628873\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 143, loss = 2.86297854\n",
      "Iteration 145, loss = 2.14175763\n",
      "Iteration 115, loss = 3.80274031\n",
      "Iteration 92, loss = 4.60273984\n",
      "Iteration 140, loss = 3.06010276\n",
      "Iteration 144, loss = 2.94542959\n",
      "Iteration 146, loss = 2.09752428\n",
      "Iteration 136, loss = 2.94714767\n",
      "Iteration 116, loss = 3.77363008\n",
      "Iteration 140, loss = 3.49700650\n",
      "Iteration 93, loss = 4.52302958\n",
      "Iteration 141, loss = 3.02472813\n",
      "Iteration 137, loss = 2.91124599\n",
      "Iteration 1, loss = 32.42123743\n",
      "Iteration 145, loss = 2.90811535\n",
      "Iteration 147, loss = 2.18362912\n",
      "Iteration 141, loss = 3.49099955\n",
      "Iteration 117, loss = 3.75880313\n",
      "Iteration 94, loss = 4.47591977\n",
      "Iteration 138, loss = 2.92796507\n",
      "Iteration 148, loss = 2.24695248\n",
      "Iteration 146, loss = 2.91223497\n",
      "Iteration 2, loss = 24.31775906\n",
      "Iteration 142, loss = 3.10635235\n",
      "Iteration 95, loss = 4.46547728\n",
      "Iteration 142, loss = 3.38270648\n",
      "Iteration 118, loss = 3.74107064\n",
      "Iteration 139, loss = 2.89988144\n",
      "Iteration 149, loss = 2.25297813\n",
      "Iteration 147, loss = 2.92542368\n",
      "Iteration 3, loss = 23.73570489\n",
      "Iteration 143, loss = 3.37616715\n",
      "Iteration 96, loss = 4.44653944\n",
      "Iteration 143, loss = 2.98415399\n",
      "Iteration 119, loss = 3.74163136\n",
      "Iteration 140, loss = 2.82990743\n",
      "Iteration 150, loss = 2.09477959\n",
      "Iteration 144, loss = 3.40588332\n",
      "Iteration 148, loss = 2.73482220\n",
      "Iteration 4, loss = 23.21035675\n",
      "Iteration 97, loss = 4.31403810\n",
      "Iteration 141, loss = 2.81119929\n",
      "Iteration 120, loss = 3.64634445\n",
      "Iteration 151, loss = 2.17123361\n",
      "Iteration 145, loss = 3.33237892\n",
      "Iteration 149, loss = 2.63910376\n",
      "Iteration 144, loss = 2.94138810\n",
      "Iteration 98, loss = 4.29359886\n",
      "Iteration 5, loss = 22.78247114\n",
      "Iteration 142, loss = 2.80171122\n",
      "Iteration 121, loss = 3.55907463\n",
      "Iteration 152, loss = 2.26969828\n",
      "Iteration 146, loss = 3.36765283\n",
      "Iteration 6, loss = 22.46481744\n",
      "Iteration 150, loss = 2.81416162\n",
      "Iteration 145, loss = 2.93940624\n",
      "Iteration 143, loss = 2.76339775\n",
      "Iteration 153, loss = 2.08298208\n",
      "Iteration 99, loss = 4.35873888\n",
      "Iteration 122, loss = 3.60969163\n",
      "Iteration 147, loss = 3.36921597\n",
      "Iteration 7, loss = 22.08514213\n",
      "Iteration 146, loss = 2.93893668\n",
      "Iteration 151, loss = 2.76128061\n",
      "Iteration 154, loss = 2.19209431\n",
      "Iteration 100, loss = 4.21776829\n",
      "Iteration 144, loss = 2.72066153\n",
      "Iteration 123, loss = 3.52837130\n",
      "Iteration 8, loss = 21.74048155\n",
      "Iteration 148, loss = 3.28964137\n",
      "Iteration 147, loss = 2.87300616\n",
      "Iteration 145, loss = 2.72733180\n",
      "Iteration 152, loss = 2.66183081\n",
      "Iteration 101, loss = 4.10375034\n",
      "Iteration 155, loss = 1.96305066\n",
      "Iteration 9, loss = 21.44571738\n",
      "Iteration 146, loss = 2.70861601\n",
      "Iteration 124, loss = 3.59775309\n",
      "Iteration 148, loss = 2.87437246\n",
      "Iteration 153, loss = 2.83911039\n",
      "Iteration 102, loss = 4.07871953\n",
      "Iteration 156, loss = 1.97453758\n",
      "Iteration 149, loss = 2.82966817\n",
      "Iteration 10, loss = 21.20147942\n",
      "Iteration 147, loss = 2.76796266\n",
      "Iteration 149, loss = 3.28295526\n",
      "Iteration 125, loss = 3.48647726\n",
      "Iteration 103, loss = 4.02145187\n",
      "Iteration 154, loss = 2.73134396\n",
      "Iteration 148, loss = 2.70497855\n",
      "Iteration 150, loss = 2.84582230\n",
      "Iteration 126, loss = 3.45914378\n",
      "Iteration 104, loss = 3.96543749\n",
      "Iteration 157, loss = 1.97331283\n",
      "Iteration 11, loss = 20.93776916\n",
      "Iteration 150, loss = 3.30059544\n",
      "Iteration 155, loss = 2.71967060\n",
      "Iteration 149, loss = 2.72905151\n",
      "Iteration 151, loss = 2.82539580\n",
      "Iteration 105, loss = 3.94458635\n",
      "Iteration 158, loss = 1.96110121\n",
      "Iteration 127, loss = 3.54994530\n",
      "Iteration 12, loss = 20.72923083\n",
      "Iteration 151, loss = 3.27212287\n",
      "Iteration 156, loss = 3.07645291\n",
      "Iteration 150, loss = 2.72391754\n",
      "Iteration 159, loss = 2.07839794\n",
      "Iteration 152, loss = 2.79420523\n",
      "Iteration 128, loss = 3.41179620\n",
      "Iteration 106, loss = 3.90488486\n",
      "Iteration 152, loss = 3.21750937\n",
      "Iteration 13, loss = 20.38820500\n",
      "Iteration 157, loss = 2.85607666\n",
      "Iteration 151, loss = 2.60439357\n",
      "Iteration 107, loss = 3.90314136\n",
      "Iteration 160, loss = 1.98392257\n",
      "Iteration 129, loss = 3.30943820\n",
      "Iteration 153, loss = 2.74672194\n",
      "Iteration 153, loss = 3.23251486\n",
      "Iteration 14, loss = 20.09350500\n",
      "Iteration 108, loss = 3.88668390\n",
      "Iteration 158, loss = 2.62145250\n",
      "Iteration 152, loss = 2.69512038\n",
      "Iteration 161, loss = 1.99427512\n",
      "Iteration 130, loss = 3.28288856\n",
      "Iteration 154, loss = 3.21629208\n",
      "Iteration 154, loss = 2.77097741\n",
      "Iteration 15, loss = 19.74202511\n",
      "Iteration 109, loss = 3.76405718\n",
      "Iteration 159, loss = 2.61030994\n",
      "Iteration 162, loss = 2.05366453\n",
      "Iteration 153, loss = 2.61065970\n",
      "Iteration 155, loss = 3.17679379\n",
      "Iteration 131, loss = 3.29172913\n",
      "Iteration 155, loss = 2.66328627\n",
      "Iteration 16, loss = 19.39679373\n",
      "Iteration 110, loss = 3.73347808\n",
      "Iteration 163, loss = 2.12331978\n",
      "Iteration 160, loss = 2.67984509\n",
      "Iteration 154, loss = 2.66033188\n",
      "Iteration 156, loss = 3.20154846\n",
      "Iteration 156, loss = 2.72679829\n",
      "Iteration 17, loss = 19.02262935\n",
      "Iteration 132, loss = 3.25840618\n",
      "Iteration 164, loss = 1.98952419\n",
      "Iteration 161, loss = 2.63933313\n",
      "Iteration 111, loss = 3.74358900\n",
      "Iteration 155, loss = 2.58186436\n",
      "Iteration 157, loss = 3.16072232\n",
      "Iteration 157, loss = 2.65004957\n",
      "Iteration 18, loss = 18.66927840\n",
      "Iteration 165, loss = 2.14365329\n",
      "Iteration 158, loss = 3.08214122\n",
      "Iteration 112, loss = 3.64740293\n",
      "Iteration 133, loss = 3.20355107\n",
      "Iteration 156, loss = 2.57002505\n",
      "Iteration 162, loss = 2.51912241\n",
      "Iteration 158, loss = 2.69792564\n",
      "Iteration 19, loss = 18.34900926\n",
      "Iteration 159, loss = 3.09998455\n",
      "Iteration 166, loss = 2.23927687\n",
      "Iteration 113, loss = 3.68913305\n",
      "Iteration 163, loss = 2.64743647\n",
      "Iteration 157, loss = 2.51156315\n",
      "Iteration 134, loss = 3.22956873\n",
      "Iteration 159, loss = 2.65084714\n",
      "Iteration 160, loss = 3.03804956\n",
      "Iteration 20, loss = 17.93434786\n",
      "Iteration 167, loss = 1.96724328\n",
      "Iteration 164, loss = 2.73772347\n",
      "Iteration 114, loss = 3.65360100\n",
      "Iteration 158, loss = 2.57227822\n",
      "Iteration 135, loss = 3.22918730\n",
      "Iteration 160, loss = 2.61262181\n",
      "Iteration 161, loss = 3.20683339\n",
      "Iteration 21, loss = 17.63154966\n",
      "Iteration 168, loss = 2.14126672\n",
      "Iteration 115, loss = 3.55351298\n",
      "Iteration 159, loss = 2.51339002\n",
      "Iteration 165, loss = 2.61723203\n",
      "Iteration 162, loss = 3.07656238\n",
      "Iteration 161, loss = 2.57691877\n",
      "Iteration 22, loss = 17.25959171\n",
      "Iteration 169, loss = 2.01819019\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 136, loss = 3.14101411\n",
      "Iteration 116, loss = 3.49677664\n",
      "Iteration 166, loss = 2.51156725\n",
      "Iteration 160, loss = 2.50028748\n",
      "Iteration 162, loss = 2.59588978\n",
      "Iteration 163, loss = 3.05020486\n",
      "Iteration 23, loss = 16.93782014\n",
      "Iteration 117, loss = 3.50347462\n",
      "Iteration 137, loss = 3.13382765\n",
      "Iteration 161, loss = 2.47238211\n",
      "Iteration 167, loss = 2.49536450\n",
      "Iteration 1, loss = 25.94309820\n",
      "Iteration 164, loss = 3.01526217\n",
      "Iteration 163, loss = 2.55960043\n",
      "Iteration 24, loss = 16.63722821\n",
      "Iteration 168, loss = 2.50191250\n",
      "Iteration 118, loss = 3.45708639\n",
      "Iteration 138, loss = 3.11959175\n",
      "Iteration 162, loss = 2.51382422\n",
      "Iteration 2, loss = 22.45863601\n",
      "Iteration 165, loss = 2.99886614\n",
      "Iteration 119, loss = 3.38938410\n",
      "Iteration 163, loss = 2.48882275\n",
      "Iteration 169, loss = 2.66879500\n",
      "Iteration 164, loss = 2.61719133\n",
      "Iteration 139, loss = 3.13757660\n",
      "Iteration 3, loss = 21.57689213\n",
      "Iteration 166, loss = 2.97416013\n",
      "Iteration 120, loss = 3.43469328\n",
      "Iteration 165, loss = 2.55484532\n",
      "Iteration 140, loss = 3.06010276\n",
      "Iteration 170, loss = 2.70228133\n",
      "Iteration 164, loss = 2.43943011\n",
      "Iteration 4, loss = 20.69115066\n",
      "Iteration 25, loss = 16.22518543\n",
      "Iteration 167, loss = 2.90188637\n",
      "Iteration 171, loss = 2.55213913\n",
      "Iteration 166, loss = 2.63259492\n",
      "Iteration 5, loss = 20.30373531\n",
      "Iteration 121, loss = 3.41739851\n",
      "Iteration 141, loss = 3.02472813\n",
      "Iteration 26, loss = 15.87834955\n",
      "Iteration 165, loss = 2.42125889\n",
      "Iteration 168, loss = 2.91364906\n",
      "Iteration 172, loss = 2.51228743\n",
      "Iteration 167, loss = 2.55315343\n",
      "Iteration 6, loss = 18.93563036\n",
      "Iteration 122, loss = 3.33042849\n",
      "Iteration 27, loss = 15.64178745\n",
      "Iteration 166, loss = 2.41781713\n",
      "Iteration 142, loss = 3.10635235\n",
      "Iteration 173, loss = 2.57382175\n",
      "Iteration 169, loss = 2.95718787\n",
      "Iteration 28, loss = 15.24589093\n",
      "Iteration 168, loss = 2.48790989\n",
      "Iteration 123, loss = 3.24381223\n",
      "Iteration 143, loss = 2.98415399\n",
      "Iteration 167, loss = 2.41572531\n",
      "Iteration 7, loss = 18.01579582\n",
      "Iteration 174, loss = 2.60364265\n",
      "Iteration 170, loss = 2.87176954\n",
      "Iteration 169, loss = 2.49430788\n",
      "Iteration 29, loss = 14.95799275\n",
      "Iteration 168, loss = 2.39778144\n",
      "Iteration 124, loss = 3.27928946\n",
      "Iteration 144, loss = 2.94138810\n",
      "Iteration 8, loss = 17.25551204\n",
      "Iteration 171, loss = 2.91709511\n",
      "Iteration 175, loss = 2.55396170\n",
      "Iteration 169, loss = 2.32803292\n",
      "Iteration 170, loss = 2.45049899\n",
      "Iteration 30, loss = 14.58781492\n",
      "Iteration 125, loss = 3.24884259\n",
      "Iteration 9, loss = 16.02499957\n",
      "Iteration 145, loss = 2.93940624\n",
      "Iteration 172, loss = 2.82729246\n",
      "Iteration 176, loss = 2.63662894\n",
      "Iteration 10, loss = 15.16440890\n",
      "Iteration 31, loss = 14.34993843\n",
      "Iteration 126, loss = 3.26243541\n",
      "Iteration 171, loss = 2.38822077\n",
      "Iteration 146, loss = 2.93893668\n",
      "Iteration 170, loss = 2.31777579\n",
      "Iteration 173, loss = 2.88890389\n",
      "Iteration 177, loss = 2.58840816\n",
      "Iteration 32, loss = 14.06312370\n",
      "Iteration 11, loss = 14.54114802\n",
      "Iteration 172, loss = 2.42957073\n",
      "Iteration 127, loss = 3.20299017\n",
      "Iteration 147, loss = 2.87300616\n",
      "Iteration 171, loss = 2.29625539\n",
      "Iteration 178, loss = 2.54782721\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 33, loss = 13.73171667\n",
      "Iteration 174, loss = 2.83950212\n",
      "Iteration 12, loss = 13.64195977\n",
      "Iteration 173, loss = 2.42666955\n",
      "Iteration 128, loss = 3.16433924\n",
      "Iteration 148, loss = 2.87437246\n",
      "Iteration 172, loss = 2.31318169\n",
      "Iteration 34, loss = 13.56257826\n",
      "Iteration 175, loss = 2.87499504\n",
      "Iteration 174, loss = 2.49256048\n",
      "Iteration 13, loss = 12.78047363\n",
      "Iteration 129, loss = 3.18935832\n",
      "Iteration 173, loss = 2.35176050\n",
      "Iteration 149, loss = 2.82966817\n",
      "Iteration 1, loss = 25.43870181\n",
      "Iteration 176, loss = 2.86927659\n",
      "Iteration 35, loss = 13.23524415\n",
      "Iteration 175, loss = 2.40663605\n",
      "Iteration 174, loss = 2.32020184\n",
      "Iteration 14, loss = 12.07857292\n",
      "Iteration 130, loss = 3.11181300\n",
      "Iteration 2, loss = 21.97425618\n",
      "Iteration 177, loss = 2.75789926\n",
      "Iteration 150, loss = 2.84582230\n",
      "Iteration 36, loss = 12.97637896\n",
      "Iteration 176, loss = 2.35739879\n",
      "Iteration 15, loss = 11.85007717\n",
      "Iteration 131, loss = 3.06340530\n",
      "Iteration 175, loss = 2.38721380\n",
      "Iteration 151, loss = 2.82539580\n",
      "Iteration 3, loss = 20.76401929\n",
      "Iteration 178, loss = 2.75792289\n",
      "Iteration 37, loss = 12.74987378\n",
      "Iteration 177, loss = 2.37084989\n",
      "Iteration 16, loss = 11.28552250\n",
      "Iteration 132, loss = 3.07892908\n",
      "Iteration 176, loss = 2.28729080\n",
      "Iteration 4, loss = 19.83262194\n",
      "Iteration 152, loss = 2.79420523\n",
      "Iteration 179, loss = 2.74722145\n",
      "Iteration 38, loss = 12.56249832\n",
      "Iteration 178, loss = 2.33950276\n",
      "Iteration 17, loss = 11.09807655\n",
      "Iteration 133, loss = 3.15892450\n",
      "Iteration 5, loss = 19.67890908\n",
      "Iteration 177, loss = 2.24101062\n",
      "Iteration 153, loss = 2.74672194\n",
      "Iteration 180, loss = 2.73492863\n",
      "Iteration 39, loss = 12.22727320\n",
      "Iteration 179, loss = 2.32338692\n",
      "Iteration 18, loss = 10.25359735\n",
      "Iteration 134, loss = 3.08952062\n",
      "Iteration 6, loss = 18.57521150\n",
      "Iteration 40, loss = 12.05548913\n",
      "Iteration 154, loss = 2.77097741\n",
      "Iteration 181, loss = 2.74389029\n",
      "Iteration 178, loss = 2.19273216\n",
      "Iteration 180, loss = 2.34165941\n",
      "Iteration 135, loss = 3.02862751\n",
      "Iteration 19, loss = 9.98602657\n",
      "Iteration 41, loss = 11.84079458\n",
      "Iteration 7, loss = 17.71061556\n",
      "Iteration 155, loss = 2.66328627\n",
      "Iteration 179, loss = 2.27737363\n",
      "Iteration 182, loss = 2.79575002\n",
      "Iteration 181, loss = 2.27673714\n",
      "Iteration 136, loss = 2.94714767\n",
      "Iteration 20, loss = 9.63684243\n",
      "Iteration 8, loss = 16.63285387\n",
      "Iteration 42, loss = 11.56014107\n",
      "Iteration 156, loss = 2.72679829\n",
      "Iteration 180, loss = 2.19427675\n",
      "Iteration 183, loss = 2.71389207\n",
      "Iteration 137, loss = 2.91124599\n",
      "Iteration 182, loss = 2.25752819\n",
      "Iteration 21, loss = 9.39440151\n",
      "Iteration 9, loss = 16.15589798\n",
      "Iteration 157, loss = 2.65004957\n",
      "Iteration 181, loss = 2.20786658\n",
      "Iteration 43, loss = 11.38844954\n",
      "Iteration 184, loss = 2.65273139\n",
      "Iteration 183, loss = 2.28031457\n",
      "Iteration 22, loss = 9.05912023\n",
      "Iteration 138, loss = 2.92796507\n",
      "Iteration 10, loss = 14.80264534\n",
      "Iteration 158, loss = 2.69792564\n",
      "Iteration 185, loss = 2.73102767\n",
      "Iteration 182, loss = 2.17170229\n",
      "Iteration 44, loss = 11.22134777\n",
      "Iteration 11, loss = 13.92007090\n",
      "Iteration 23, loss = 8.51356308\n",
      "Iteration 139, loss = 2.89988144\n",
      "Iteration 184, loss = 2.23434067\n",
      "Iteration 159, loss = 2.65084714\n",
      "Iteration 186, loss = 2.76267335\n",
      "Iteration 45, loss = 10.96524404\n",
      "Iteration 183, loss = 2.17430631\n",
      "Iteration 24, loss = 8.42520964\n",
      "Iteration 12, loss = 13.45981687\n",
      "Iteration 140, loss = 2.82990743\n",
      "Iteration 185, loss = 2.25767907\n",
      "Iteration 160, loss = 2.61262181\n",
      "Iteration 187, loss = 2.71381324\n",
      "Iteration 46, loss = 10.76194855\n",
      "Iteration 25, loss = 8.25169607\n",
      "Iteration 184, loss = 2.19299362\n",
      "Iteration 13, loss = 12.88710572\n",
      "Iteration 141, loss = 2.81119929\n",
      "Iteration 161, loss = 2.57691877\n",
      "Iteration 186, loss = 2.23939571\n",
      "Iteration 47, loss = 10.67487841\n",
      "Iteration 14, loss = 11.94013500\n",
      "Iteration 26, loss = 7.95126957\n",
      "Iteration 188, loss = 2.64103268\n",
      "Iteration 185, loss = 2.13365274\n",
      "Iteration 142, loss = 2.80171122\n",
      "Iteration 187, loss = 2.21926362\n",
      "Iteration 48, loss = 10.35706332\n",
      "Iteration 189, loss = 2.63726505\n",
      "Iteration 162, loss = 2.59588978\n",
      "Iteration 27, loss = 7.89704442\n",
      "Iteration 186, loss = 2.22855964\n",
      "Iteration 15, loss = 11.21264198\n",
      "Iteration 190, loss = 2.61363523\n",
      "Iteration 188, loss = 2.22977604\n",
      "Iteration 49, loss = 10.22346103\n",
      "Iteration 163, loss = 2.55960043\n",
      "Iteration 28, loss = 7.68129268\n",
      "Iteration 143, loss = 2.76339775\n",
      "Iteration 16, loss = 10.84205029\n",
      "Iteration 187, loss = 2.13575760\n",
      "Iteration 50, loss = 10.00370199\n",
      "Iteration 191, loss = 2.64818859\n",
      "Iteration 164, loss = 2.61719133\n",
      "Iteration 189, loss = 2.16859013\n",
      "Iteration 29, loss = 7.68400556\n",
      "Iteration 144, loss = 2.72066153\n",
      "Iteration 17, loss = 10.47435532\n",
      "Iteration 192, loss = 2.59358861\n",
      "Iteration 188, loss = 2.23172827\n",
      "Iteration 51, loss = 9.79619769\n",
      "Iteration 165, loss = 2.55484532\n",
      "Iteration 190, loss = 2.18934941\n",
      "Iteration 30, loss = 7.20049628\n",
      "Iteration 145, loss = 2.72733180\n",
      "Iteration 18, loss = 10.01053986\n",
      "Iteration 193, loss = 2.62703172\n",
      "Iteration 166, loss = 2.63259492\n",
      "Iteration 189, loss = 2.11338552\n",
      "Iteration 52, loss = 9.63572402\n",
      "Iteration 191, loss = 2.20892573\n",
      "Iteration 31, loss = 7.01751764\n",
      "Iteration 19, loss = 9.61003901\n",
      "Iteration 194, loss = 2.55916031\n",
      "Iteration 146, loss = 2.70861601\n",
      "Iteration 190, loss = 2.10206363\n",
      "Iteration 167, loss = 2.55315343\n",
      "Iteration 53, loss = 9.50956800\n",
      "Iteration 192, loss = 2.12159086\n",
      "Iteration 32, loss = 6.93794428\n",
      "Iteration 195, loss = 2.55987738\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m mlp_param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_layer_sizes\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m400\u001b[39m, \u001b[38;5;241m500\u001b[39m],\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvscaling\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madaptive\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate_init\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.1\u001b[39m],\n\u001b[1;32m      5\u001b[0m }\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmake_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregressors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMLP\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_param_grid\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 11\u001b[0m, in \u001b[0;36mmake_prediction\u001b[0;34m(Regressor, param_grid)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_prediction\u001b[39m(Regressor, param_grid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m      5\u001b[0m     regr \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      6\u001b[0m         GridSearchCV(Regressor, param_grid, scoring\u001b[38;5;241m=\u001b[39mrmse_scorer, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m param_grid\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m Regressor\n\u001b[1;32m      9\u001b[0m     )\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mregr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param_grid:\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregr\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Studies/SI/SIwIB/.venv/lib/python3.9/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Studies/SI/SIwIB/.venv/lib/python3.9/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Studies/SI/SIwIB/.venv/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1527\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Studies/SI/SIwIB/.venv/lib/python3.9/site-packages/sklearn/model_selection/_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    913\u001b[0m         )\n\u001b[1;32m    914\u001b[0m     )\n\u001b[0;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    939\u001b[0m     )\n",
      "File \u001b[0;32m~/Studies/SI/SIwIB/.venv/lib/python3.9/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Studies/SI/SIwIB/.venv/lib/python3.9/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Studies/SI/SIwIB/.venv/lib/python3.9/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Studies/SI/SIwIB/.venv/lib/python3.9/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mlp_param_grid = {\n",
    "    \"hidden_layer_sizes\": [100, 200, 300, 400, 500],\n",
    "    \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"],\n",
    "    \"learning_rate_init\": [0.001, 0.01, 0.1],\n",
    "}\n",
    "\n",
    "make_prediction(regressors[\"MLP\"], mlp_param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
